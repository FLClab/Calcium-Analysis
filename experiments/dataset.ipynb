{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e27307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold train 58\n",
      "positive samples 9532\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_samples 58\n",
      "fold valid 15\n",
      "positive samples 2639\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_samples 58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_samples 58\n",
      "train 2440192\n",
      "train 2440192\n",
      "train 2440192\n",
      "train 2440192\n",
      "train 2440192\n",
      "train 2440192\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "import h5py\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "from skimage import filters\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MAX_PU = 8\n",
    "PATH = \"../data/80-20_calcium_dataset.h5\"\n",
    "numpy.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "POSITIVE_SAMPLES = {}\n",
    "NEGATIVE_SAMPLES = defaultdict(list)\n",
    "with h5py.File(PATH, \"r\") as file:\n",
    "    for fold, fold_values in tqdm(file.items(), leave=False):\n",
    "        print('fold', fold, len(fold_values.keys()))\n",
    "        positive_samples = []\n",
    "        for neuron, neuron_values in fold_values.items():\n",
    "            for idx, event in enumerate(neuron_values[\"events\"]):\n",
    "                positive_samples.append({\n",
    "                    \"neuron\" : neuron,\n",
    "                    \"event-id\" : idx\n",
    "                })\n",
    "        POSITIVE_SAMPLES[fold] = positive_samples\n",
    "        print('positive samples',len(POSITIVE_SAMPLES[fold]))\n",
    "        \n",
    "        if fold == \"train\":\n",
    "            for neuron, neuron_values in tqdm(fold_values.items(), leave=False):\n",
    "                num_samples = int(len(neuron_values[\"events\"]) * MAX_PU)\n",
    "                max_proj = neuron_values[\"input\"][:25].mean(axis=0) # keeps first N frames to compute foreground\n",
    "                foreground = max_proj > filters.threshold_triangle(max_proj)\n",
    "                mask = numpy.zeros_like(foreground, dtype=int)\n",
    "                mask[64 : -64, 64 : -64] = 1\n",
    "                foreground = foreground.astype(int) * mask.astype(int)\n",
    "                indices = numpy.argwhere(foreground)\n",
    "                choices = numpy.random.choice(len(indices), size=num_samples, replace=False)\n",
    "                yx = indices[choices]\n",
    "                t = numpy.random.randint(64, neuron_values[\"input\"].shape[0]-64, size=len(choices))\n",
    "                coords = numpy.concatenate((t[:, numpy.newaxis], yx), axis=-1)\n",
    "                negative_samples = []\n",
    "                for coord in coords:\n",
    "                    negative_samples.append({\n",
    "                        \"neuron\" : neuron,\n",
    "                        \"coord\" : coord.tolist()\n",
    "                    })\n",
    "                NEGATIVE_SAMPLES[fold].append(negative_samples)\n",
    "            print('negative_samples',len(NEGATIVE_SAMPLES[fold]))\n",
    "\n",
    "numpy.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "MAX_PU_64 = 64\n",
    "NEGATIVE_SAMPLES_64 = defaultdict(list)\n",
    "with h5py.File(PATH, \"r\") as file:\n",
    "    for fold, fold_values in tqdm(file.items(), leave=False):\n",
    "        if fold == \"train\":\n",
    "            for i, (neuron, neuron_values) in enumerate(tqdm(fold_values.items(), leave=False)):\n",
    "                num_samples = int(len(neuron_values[\"events\"]) * MAX_PU_64) - len(NEGATIVE_SAMPLES[fold][i])\n",
    "#                 print(num_samples)\n",
    "                max_proj = neuron_values[\"input\"][:25].mean(axis=0) # keeps first N frames to compute foreground\n",
    "                foreground = max_proj > filters.threshold_triangle(max_proj)\n",
    "                mask = numpy.zeros_like(foreground, dtype=int)\n",
    "                mask[64 : -64, 64 : -64] = 1\n",
    "                foreground = foreground.astype(int) * mask.astype(int)\n",
    "                indices = numpy.argwhere(foreground)\n",
    "                choices = numpy.random.choice(len(indices), size=num_samples, replace=num_samples > len(indices))\n",
    "                yx = indices[choices]\n",
    "                t = numpy.random.randint(64, neuron_values[\"input\"].shape[0]-64, size=len(choices))\n",
    "                coords = numpy.concatenate((t[:, numpy.newaxis], yx), axis=-1)\n",
    "                negative_samples = NEGATIVE_SAMPLES[fold][i]\n",
    "                for coord in coords:\n",
    "                    negative_samples.append({\n",
    "                        \"neuron\" : neuron,\n",
    "                        \"coord\" : coord.tolist()\n",
    "                    })\n",
    "                NEGATIVE_SAMPLES_64[fold].append(negative_samples)\n",
    "            print('negative_samples',len(NEGATIVE_SAMPLES_64[fold]))\n",
    "            \n",
    "MAX_PU_256 = 256\n",
    "NEGATIVE_SAMPLES_256 = defaultdict(list)\n",
    "with h5py.File(PATH, \"r\") as file:\n",
    "    for fold, fold_values in tqdm(file.items(), leave=False):\n",
    "        if fold == \"train\":\n",
    "            for i, (neuron, neuron_values) in enumerate(tqdm(fold_values.items(), leave=False)):\n",
    "                num_samples = int(len(neuron_values[\"events\"]) * MAX_PU_256) - len(NEGATIVE_SAMPLES_64[fold][i])\n",
    "#                 print(num_samples)\n",
    "                max_proj = neuron_values[\"input\"][:25].mean(axis=0) # keeps first N frames to compute foreground\n",
    "                foreground = max_proj > filters.threshold_triangle(max_proj)\n",
    "                mask = numpy.zeros_like(foreground, dtype=int)\n",
    "                mask[64 : -64, 64 : -64] = 1\n",
    "                foreground = foreground.astype(int) * mask.astype(int)\n",
    "                indices = numpy.argwhere(foreground)\n",
    "                choices = numpy.random.choice(len(indices), size=num_samples, replace=num_samples > len(indices))\n",
    "                yx = indices[choices]\n",
    "                t = numpy.random.randint(64, neuron_values[\"input\"].shape[0]-64, size=len(choices))\n",
    "                coords = numpy.concatenate((t[:, numpy.newaxis], yx), axis=-1)\n",
    "                negative_samples = NEGATIVE_SAMPLES_64[fold][i]\n",
    "                for coord in coords:\n",
    "                    negative_samples.append({\n",
    "                        \"neuron\" : neuron,\n",
    "                        \"coord\" : coord.tolist()\n",
    "                    })\n",
    "                NEGATIVE_SAMPLES_256[fold].append(negative_samples)\n",
    "            print('negative_samples',len(NEGATIVE_SAMPLES_256[fold]))            \n",
    "\n",
    "for key, values in NEGATIVE_SAMPLES.items():\n",
    "    print(key, sum([len(value) for value in values]))\n",
    "    print(key, len(flatten(values)))\n",
    "    \n",
    "for key, values in NEGATIVE_SAMPLES_64.items():\n",
    "    print(key, sum([len(value) for value in values]))\n",
    "    print(key, len(flatten(values)))    \n",
    "    \n",
    "for key, values in NEGATIVE_SAMPLES_256.items():\n",
    "    print(key, sum([len(value) for value in values]))\n",
    "    print(key, len(flatten(values)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab696ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(items):\n",
    "    \"\"\"\n",
    "    Recursively flattens a list of items\n",
    "    \"\"\"\n",
    "    flattened = []\n",
    "    if isinstance(items, (dict, float, int)):\n",
    "        return [items]\n",
    "    else:\n",
    "        for item in items:\n",
    "            flattened.extend(flatten(item))\n",
    "    return flattened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3830579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9532\n",
      "0\n",
      "9532\n",
      "2639\n",
      "2440192\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Keeps 7800 positive crops\n",
    "tmp_updated_samples = {}\n",
    "for fold, values in POSITIVE_SAMPLES.items():\n",
    "    tmp_updated_samples[fold] = values\n",
    "updated_samples = copy.deepcopy(tmp_updated_samples)\n",
    "\n",
    "SAMPLES_PU = {\n",
    "    \"complete-1:0\" : {\n",
    "        \"positive\" : updated_samples,\n",
    "        \"negative\" : {\"train\" : [], \"valid\" : []}\n",
    "    }\n",
    "}\n",
    "\n",
    "tmp_updated_samples = {}\n",
    "subset = []\n",
    "for neuron_subset in NEGATIVE_SAMPLES[\"train\"]:\n",
    "    subset.append(neuron_subset)\n",
    "for i, ratio in enumerate([8, 4, 2, 1]):\n",
    "    tmp_subset = []\n",
    "    for s in subset:\n",
    "        choices = numpy.random.choice(s, size=len(s) if ratio == 8 else len(s) // 2, replace=False)\n",
    "        tmp_subset.append(choices.tolist())\n",
    "    tmp_updated_samples[ratio] = flatten(tmp_subset)\n",
    "    subset = copy.deepcopy(tmp_subset)\n",
    "\n",
    "for key, values in tmp_updated_samples.items():\n",
    "    SAMPLES_PU[f\"complete-1:{key}\"] = {\n",
    "        \"positive\" : updated_samples,\n",
    "        \"negative\" : {\n",
    "            \"train\" : tmp_updated_samples[key],\n",
    "            \"valid\" : [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(len(SAMPLES_PU[\"complete-1:0\"][\"positive\"][\"train\"]))    \n",
    "print(len(SAMPLES_PU[\"complete-1:0\"][\"negative\"][\"train\"]))\n",
    "\n",
    "print(len(SAMPLES_PU[\"complete-1:4\"][\"positive\"][\"train\"]))\n",
    "print(len(SAMPLES_PU[\"complete-1:4\"][\"positive\"][\"valid\"]))\n",
    "\n",
    "print(len(SAMPLES_PU[\"complete-1:8\"][\"negative\"][\"train\"]))\n",
    "print(len(SAMPLES_PU[\"complete-1:8\"][\"negative\"][\"valid\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1d454230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(SAMPLES_PU, open(\"../configs/training-samples_complete.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7c44fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152512\n",
      "0\n",
      "305024\n",
      "0\n",
      "610048\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Keeps 7800 positive crops\n",
    "tmp_updated_samples = {}\n",
    "for fold, values in POSITIVE_SAMPLES.items():\n",
    "    tmp_updated_samples[fold] = values\n",
    "updated_samples = copy.deepcopy(tmp_updated_samples)\n",
    "\n",
    "SAMPLES_PU = {\n",
    "    \"complete-1:0\" : {\n",
    "        \"positive\" : updated_samples,\n",
    "        \"negative\" : {\"train\" : [], \"valid\" : []}\n",
    "    }\n",
    "}\n",
    "\n",
    "tmp_updated_samples = {}\n",
    "subset = []\n",
    "for neuron_subset in NEGATIVE_SAMPLES_64[\"train\"]:\n",
    "    subset.append(neuron_subset)\n",
    "for i, ratio in enumerate([64, 32, 16]):\n",
    "    tmp_subset = []\n",
    "    for s in subset:\n",
    "        choices = numpy.random.choice(s, size=len(s) if ratio == 64 else len(s) // 2, replace=False)\n",
    "        tmp_subset.append(choices.tolist())\n",
    "    tmp_updated_samples[ratio] = flatten(tmp_subset)\n",
    "    subset = copy.deepcopy(tmp_subset)\n",
    "\n",
    "for key, values in tmp_updated_samples.items():\n",
    "    SAMPLES_PU[f\"complete-1:{key}\"] = {\n",
    "        \"positive\" : updated_samples,\n",
    "        \"negative\" : {\n",
    "            \"train\" : tmp_updated_samples[key],\n",
    "            \"valid\" : [],\n",
    "        }\n",
    "    }\n",
    "\n",
    "# print(len(SAMPLES_PU[\"complete-1:4\"][\"positive\"][\"train\"]))\n",
    "# print(len(SAMPLES_PU[\"complete-1:4\"][\"positive\"][\"valid\"]))\n",
    "\n",
    "# print(len(SAMPLES_PU[\"complete-1:8\"][\"negative\"][\"train\"]))\n",
    "# print(len(SAMPLES_PU[\"complete-1:8\"][\"negative\"][\"valid\"]))\n",
    "\n",
    "print(len(SAMPLES_PU[\"complete-1:16\"][\"negative\"][\"train\"]))\n",
    "print(len(SAMPLES_PU[\"complete-1:16\"][\"negative\"][\"valid\"]))\n",
    "\n",
    "print(len(SAMPLES_PU[\"complete-1:32\"][\"negative\"][\"train\"]))\n",
    "print(len(SAMPLES_PU[\"complete-1:32\"][\"negative\"][\"valid\"]))\n",
    "\n",
    "print(len(SAMPLES_PU[\"complete-1:64\"][\"negative\"][\"train\"]))\n",
    "print(len(SAMPLES_PU[\"complete-1:64\"][\"negative\"][\"valid\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "df903666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(SAMPLES_PU, open(\"../configs/training-samples_complete_64.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8e71e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1220096\n",
      "0\n",
      "2440192\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Keeps 7800 positive crops\n",
    "tmp_updated_samples = {}\n",
    "for fold, values in POSITIVE_SAMPLES.items():\n",
    "    tmp_updated_samples[fold] = values\n",
    "updated_samples = copy.deepcopy(tmp_updated_samples)\n",
    "\n",
    "SAMPLES_PU = {\n",
    "    \"complete-1:0\" : {\n",
    "        \"positive\" : updated_samples,\n",
    "        \"negative\" : {\"train\" : [], \"valid\" : []}\n",
    "    }\n",
    "}\n",
    "\n",
    "tmp_updated_samples = {}\n",
    "subset = []\n",
    "for neuron_subset in NEGATIVE_SAMPLES_256[\"train\"]:\n",
    "    subset.append(neuron_subset)\n",
    "for i, ratio in enumerate([256, 128]):\n",
    "    tmp_subset = []\n",
    "    for s in subset:\n",
    "        choices = numpy.random.choice(s, size=len(s) if ratio == 256 else len(s) // 2, replace=False)\n",
    "        tmp_subset.append(choices.tolist())\n",
    "    tmp_updated_samples[ratio] = flatten(tmp_subset)\n",
    "    subset = copy.deepcopy(tmp_subset)\n",
    "\n",
    "for key, values in tmp_updated_samples.items():\n",
    "    SAMPLES_PU[f\"complete-1:{key}\"] = {\n",
    "        \"positive\" : updated_samples,\n",
    "        \"negative\" : {\n",
    "            \"train\" : tmp_updated_samples[key],\n",
    "            \"valid\" : [],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "print(len(SAMPLES_PU[\"complete-1:128\"][\"negative\"][\"train\"]))\n",
    "print(len(SAMPLES_PU[\"complete-1:128\"][\"negative\"][\"valid\"]))\n",
    "\n",
    "print(len(SAMPLES_PU[\"complete-1:256\"][\"negative\"][\"train\"]))\n",
    "print(len(SAMPLES_PU[\"complete-1:256\"][\"negative\"][\"valid\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4660b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(SAMPLES_PU, open(\"../configs/training-samples_complete_256.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb406da6",
   "metadata": {},
   "source": [
    "# Subset PU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "194d8293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "SAMPLES_PU = json.load(open(\"../configs/training-samples_complete.json\", \"r\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8520efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEURONS = list(set([sample[\"neuron\"] for sample in SAMPLES_PU[\"complete-1:0\"][\"positive\"][\"train\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db0196cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "def filter_neurons(samples_pu, subset):\n",
    "    samples_pu = copy.deepcopy(samples_pu)\n",
    "    \n",
    "    out_pu = {}\n",
    "    for key_pu, values_pu in samples_pu.items():\n",
    "        key_pu = key_pu.replace(\"complete\", \"subset\")\n",
    "        out_pu[key_pu] = {}\n",
    "        for key_type, values_type in values_pu.items():\n",
    "            out_pu[key_pu][key_type] = {}\n",
    "            for key_dataset, values_dataset in values_type.items():\n",
    "                if key_dataset == \"train\":\n",
    "                    out_pu[key_pu][key_type][key_dataset] = [value for value in values_dataset if value[\"neuron\"] in subset]\n",
    "                else:\n",
    "                    out_pu[key_pu][key_type][key_dataset] = values_dataset\n",
    "    return out_pu\n",
    "\n",
    "SUBSET_SAMPLES_PU = {}\n",
    "for subset_factor in [0.25, 0.5, 0.75]:\n",
    "    for repetition in range(5):\n",
    "        choices = numpy.random.choice(NEURONS, size=int(len(NEURONS) * subset_factor), replace=False)\n",
    "        filtered = filter_neurons(SAMPLES_PU, choices)\n",
    "        \n",
    "        for key, values in filtered.items():\n",
    "            key = key.replace(\"subset\", f\"subset-{subset_factor}-{repetition}\")\n",
    "            SUBSET_SAMPLES_PU[key] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6976d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(SUBSET_SAMPLES_PU, open(\"../configs/training-samples_subset.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "900d0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "complete_sums = {}\n",
    "for key, values in SAMPLES_PU.items():\n",
    "    running_sum = 0\n",
    "    for key_type, values_type in values.items():\n",
    "        running_sum += len(values_type[\"train\"])\n",
    "    complete_sums[key] = running_sum\n",
    "\n",
    "subset_sums = {}\n",
    "for key, values in SUBSET_SAMPLES_PU.items():\n",
    "    running_sum = []\n",
    "    for key_type, values_type in values.items():\n",
    "        running_sum.append(len(values_type[\"train\"]))\n",
    "    subset_sums[key] = running_sum\n",
    "\n",
    "for subset_factor in [0.25, 0.5, 0.75]:\n",
    "    for repetition in range(5):\n",
    "        for ratio in [0, 1, 2, 4, 8]:\n",
    "            if sum(subset_sums[f\"subset-{subset_factor}-{repetition}-1:{ratio}\"]) > complete_sums[\"complete-1:0\"]:\n",
    "                break\n",
    "                \n",
    "        # total positive - num positive \n",
    "        sample_num = complete_sums[\"complete-1:0\"] - subset_sums[f\"subset-{subset_factor}-{repetition}-1:{ratio}\"][0]\n",
    "\n",
    "        SUBSET_SAMPLES_PU[f\"subset-{subset_factor}-{repetition}-1:c\"] = {\n",
    "            \"positive\" : {\n",
    "                \"train\" : SUBSET_SAMPLES_PU[f\"subset-{subset_factor}-{repetition}-1:{ratio}\"][\"positive\"][\"train\"],\n",
    "                \"valid\" : SUBSET_SAMPLES_PU[f\"subset-{subset_factor}-{repetition}-1:{ratio}\"][\"positive\"][\"valid\"],\n",
    "            },\n",
    "            \"negative\" : {\n",
    "                \"train\" : numpy.random.choice(\n",
    "                    SUBSET_SAMPLES_PU[f\"subset-{subset_factor}-{repetition}-1:{ratio}\"][\"negative\"][\"train\"],\n",
    "                    size = min(sample_num, len(SUBSET_SAMPLES_PU[f\"subset-{subset_factor}-{repetition}-1:{ratio}\"][\"negative\"][\"train\"])),\n",
    "                    replace = False\n",
    "                ).tolist(),\n",
    "                \"valid\" : SUBSET_SAMPLES_PU[f\"subset-{subset_factor}-{repetition}-1:{ratio}\"][\"negative\"][\"valid\"],\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "6a40ace5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset-0.25-0-1:c 9532\n",
      "subset-0.25-1-1:c 9522\n",
      "subset-0.25-2-1:c 9532\n",
      "subset-0.25-3-1:c 9532\n",
      "subset-0.25-4-1:c 9532\n",
      "subset-0.5-0-1:c 9532\n",
      "subset-0.5-1-1:c 9532\n",
      "subset-0.5-2-1:c 9532\n",
      "subset-0.5-3-1:c 9532\n",
      "subset-0.5-4-1:c 9532\n",
      "subset-0.75-0-1:c 9532\n",
      "subset-0.75-1-1:c 9532\n",
      "subset-0.75-2-1:c 9532\n",
      "subset-0.75-3-1:c 9532\n",
      "subset-0.75-4-1:c 9532\n"
     ]
    }
   ],
   "source": [
    "subset_sums = {}\n",
    "for key, values in SUBSET_SAMPLES_PU.items():\n",
    "    running_sum = []\n",
    "    for key_type, values_type in values.items():\n",
    "        running_sum.append(len(values_type[\"train\"]))\n",
    "    subset_sums[key] = running_sum\n",
    "for key, values in subset_sums.items():\n",
    "    if \"1:c\" in key:\n",
    "        print(key, sum(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e9b88672",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(SUBSET_SAMPLES_PU, open(\"../configs/training-samples_subset.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3b49b7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subset-0.25-0-1:0 3319 2639 0 0\n",
      "subset-0.25-0-1:8 3319 2639 26552 0\n",
      "subset-0.25-0-1:4 3319 2639 13276 0\n",
      "subset-0.25-0-1:2 3319 2639 6638 0\n",
      "subset-0.25-0-1:1 3319 2639 3319 0\n",
      "subset-0.25-1-1:0 1058 2639 0 0\n",
      "subset-0.25-1-1:8 1058 2639 8464 0\n",
      "subset-0.25-1-1:4 1058 2639 4232 0\n",
      "subset-0.25-1-1:2 1058 2639 2116 0\n",
      "subset-0.25-1-1:1 1058 2639 1058 0\n",
      "subset-0.25-2-1:0 1359 2639 0 0\n",
      "subset-0.25-2-1:8 1359 2639 10872 0\n",
      "subset-0.25-2-1:4 1359 2639 5436 0\n",
      "subset-0.25-2-1:2 1359 2639 2718 0\n",
      "subset-0.25-2-1:1 1359 2639 1359 0\n",
      "subset-0.25-3-1:0 1427 2639 0 0\n",
      "subset-0.25-3-1:8 1427 2639 11416 0\n",
      "subset-0.25-3-1:4 1427 2639 5708 0\n",
      "subset-0.25-3-1:2 1427 2639 2854 0\n",
      "subset-0.25-3-1:1 1427 2639 1427 0\n",
      "subset-0.25-4-1:0 3066 2639 0 0\n",
      "subset-0.25-4-1:8 3066 2639 24528 0\n",
      "subset-0.25-4-1:4 3066 2639 12264 0\n",
      "subset-0.25-4-1:2 3066 2639 6132 0\n",
      "subset-0.25-4-1:1 3066 2639 3066 0\n",
      "subset-0.5-0-1:0 4094 2639 0 0\n",
      "subset-0.5-0-1:8 4094 2639 32752 0\n",
      "subset-0.5-0-1:4 4094 2639 16376 0\n",
      "subset-0.5-0-1:2 4094 2639 8188 0\n",
      "subset-0.5-0-1:1 4094 2639 4094 0\n",
      "subset-0.5-1-1:0 5156 2639 0 0\n",
      "subset-0.5-1-1:8 5156 2639 41248 0\n",
      "subset-0.5-1-1:4 5156 2639 20624 0\n",
      "subset-0.5-1-1:2 5156 2639 10312 0\n",
      "subset-0.5-1-1:1 5156 2639 5156 0\n",
      "subset-0.5-2-1:0 4897 2639 0 0\n",
      "subset-0.5-2-1:8 4897 2639 39176 0\n",
      "subset-0.5-2-1:4 4897 2639 19588 0\n",
      "subset-0.5-2-1:2 4897 2639 9794 0\n",
      "subset-0.5-2-1:1 4897 2639 4897 0\n",
      "subset-0.5-3-1:0 4753 2639 0 0\n",
      "subset-0.5-3-1:8 4753 2639 38024 0\n",
      "subset-0.5-3-1:4 4753 2639 19012 0\n",
      "subset-0.5-3-1:2 4753 2639 9506 0\n",
      "subset-0.5-3-1:1 4753 2639 4753 0\n",
      "subset-0.5-4-1:0 5520 2639 0 0\n",
      "subset-0.5-4-1:8 5520 2639 44160 0\n",
      "subset-0.5-4-1:4 5520 2639 22080 0\n",
      "subset-0.5-4-1:2 5520 2639 11040 0\n",
      "subset-0.5-4-1:1 5520 2639 5520 0\n",
      "subset-0.75-0-1:0 6472 2639 0 0\n",
      "subset-0.75-0-1:8 6472 2639 51776 0\n",
      "subset-0.75-0-1:4 6472 2639 25888 0\n",
      "subset-0.75-0-1:2 6472 2639 12944 0\n",
      "subset-0.75-0-1:1 6472 2639 6472 0\n",
      "subset-0.75-1-1:0 6564 2639 0 0\n",
      "subset-0.75-1-1:8 6564 2639 52512 0\n",
      "subset-0.75-1-1:4 6564 2639 26256 0\n",
      "subset-0.75-1-1:2 6564 2639 13128 0\n",
      "subset-0.75-1-1:1 6564 2639 6564 0\n",
      "subset-0.75-2-1:0 7646 2639 0 0\n",
      "subset-0.75-2-1:8 7646 2639 61168 0\n",
      "subset-0.75-2-1:4 7646 2639 30584 0\n",
      "subset-0.75-2-1:2 7646 2639 15292 0\n",
      "subset-0.75-2-1:1 7646 2639 7646 0\n",
      "subset-0.75-3-1:0 7249 2639 0 0\n",
      "subset-0.75-3-1:8 7249 2639 57992 0\n",
      "subset-0.75-3-1:4 7249 2639 28996 0\n",
      "subset-0.75-3-1:2 7249 2639 14498 0\n",
      "subset-0.75-3-1:1 7249 2639 7249 0\n",
      "subset-0.75-4-1:0 7617 2639 0 0\n",
      "subset-0.75-4-1:8 7617 2639 60936 0\n",
      "subset-0.75-4-1:4 7617 2639 30468 0\n",
      "subset-0.75-4-1:2 7617 2639 15234 0\n",
      "subset-0.75-4-1:1 7617 2639 7617 0\n",
      "subset-0.25-0-1:c 3319 2639 6213 0\n",
      "subset-0.25-1-1:c 1058 2639 8464 0\n",
      "subset-0.25-2-1:c 1359 2639 8173 0\n",
      "subset-0.25-3-1:c 1427 2639 8105 0\n",
      "subset-0.25-4-1:c 3066 2639 6466 0\n",
      "subset-0.5-0-1:c 4094 2639 5438 0\n",
      "subset-0.5-1-1:c 5156 2639 4376 0\n",
      "subset-0.5-2-1:c 4897 2639 4635 0\n",
      "subset-0.5-3-1:c 4753 2639 4779 0\n",
      "subset-0.5-4-1:c 5520 2639 4012 0\n",
      "subset-0.75-0-1:c 6472 2639 3060 0\n",
      "subset-0.75-1-1:c 6564 2639 2968 0\n",
      "subset-0.75-2-1:c 7646 2639 1886 0\n",
      "subset-0.75-3-1:c 7249 2639 2283 0\n",
      "subset-0.75-4-1:c 7617 2639 1915 0\n"
     ]
    }
   ],
   "source": [
    "for key, values in SUBSET_SAMPLES_PU.items():\n",
    "    print(key, len(values[\"positive\"][\"train\"]), len(values[\"positive\"][\"valid\"]), len(values[\"negative\"][\"train\"]), len(values[\"negative\"][\"valid\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431a9b9",
   "metadata": {},
   "source": [
    "## Subset-0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7a83f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "SAMPLES_PU = json.load(open(\"../configs/training-samples_complete_64.json\", \"r\"))\n",
    "NEURONS = list(set([sample[\"neuron\"] for sample in SAMPLES_PU[\"complete-1:0\"][\"positive\"][\"train\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "378683ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "def filter_neurons(samples_pu, subset):\n",
    "    samples_pu = copy.deepcopy(samples_pu)\n",
    "    \n",
    "    out_pu = {}\n",
    "    for key_pu, values_pu in samples_pu.items():\n",
    "        if \"1:0\" in key_pu:\n",
    "            continue\n",
    "        key_pu = key_pu.replace(\"complete\", \"subset\")\n",
    "        out_pu[key_pu] = {}\n",
    "        for key_type, values_type in values_pu.items():\n",
    "            out_pu[key_pu][key_type] = {}\n",
    "            for key_dataset, values_dataset in values_type.items():\n",
    "                if key_dataset == \"train\":\n",
    "                    out_pu[key_pu][key_type][key_dataset] = [value for value in values_dataset if value[\"neuron\"] in subset]\n",
    "                else:\n",
    "                    out_pu[key_pu][key_type][key_dataset] = values_dataset\n",
    "    return out_pu\n",
    "\n",
    "SUBSET_SAMPLES_PU = {}\n",
    "for subset_factor in [0.25]:\n",
    "    for repetition in range(5):\n",
    "        choices = numpy.random.choice(NEURONS, size=int(len(NEURONS) * subset_factor), replace=False)\n",
    "        filtered = filter_neurons(SAMPLES_PU, choices)\n",
    "        \n",
    "        for key, values in filtered.items():\n",
    "            key = key.replace(\"subset\", f\"subset-{subset_factor}-{repetition}\")\n",
    "            SUBSET_SAMPLES_PU[key] = values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f9897e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['subset-0.25-0-1:64', 'subset-0.25-0-1:32', 'subset-0.25-0-1:16', 'subset-0.25-1-1:64', 'subset-0.25-1-1:32', 'subset-0.25-1-1:16', 'subset-0.25-2-1:64', 'subset-0.25-2-1:32', 'subset-0.25-2-1:16', 'subset-0.25-3-1:64', 'subset-0.25-3-1:32', 'subset-0.25-3-1:16', 'subset-0.25-4-1:64', 'subset-0.25-4-1:32', 'subset-0.25-4-1:16'])\n"
     ]
    }
   ],
   "source": [
    "print(SUBSET_SAMPLES_PU.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81038fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(SUBSET_SAMPLES_PU, open(\"../configs/training-samples_subset_64.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99f0139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "SAMPLES_PU = json.load(open(\"../configs/training-samples_complete_256.json\", \"r\"))\n",
    "NEURONS = list(set([sample[\"neuron\"] for sample in SAMPLES_PU[\"complete-1:0\"][\"positive\"][\"train\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96e306c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "def filter_neurons(samples_pu, subset):\n",
    "    samples_pu = copy.deepcopy(samples_pu)\n",
    "    \n",
    "    out_pu = {}\n",
    "    for key_pu, values_pu in samples_pu.items():\n",
    "        if \"1:0\" in key_pu:\n",
    "            continue\n",
    "        key_pu = key_pu.replace(\"complete\", \"subset\")\n",
    "        out_pu[key_pu] = {}\n",
    "        for key_type, values_type in values_pu.items():\n",
    "            out_pu[key_pu][key_type] = {}\n",
    "            for key_dataset, values_dataset in values_type.items():\n",
    "                if key_dataset == \"train\":\n",
    "                    out_pu[key_pu][key_type][key_dataset] = [value for value in values_dataset if value[\"neuron\"] in subset]\n",
    "                else:\n",
    "                    out_pu[key_pu][key_type][key_dataset] = values_dataset\n",
    "    return out_pu\n",
    "\n",
    "SUBSET_SAMPLES_PU = {}\n",
    "for subset_factor in [0.25]:\n",
    "    for repetition in range(5):\n",
    "        choices = numpy.random.choice(NEURONS, size=int(len(NEURONS) * subset_factor), replace=False)\n",
    "        filtered = filter_neurons(SAMPLES_PU, choices)\n",
    "        \n",
    "        for key, values in filtered.items():\n",
    "            key = key.replace(\"subset\", f\"subset-{subset_factor}-{repetition}\")\n",
    "            SUBSET_SAMPLES_PU[key] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98e66423",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(SUBSET_SAMPLES_PU, open(\"../configs/training-samples_subset_256.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a85e375",
   "metadata": {},
   "source": [
    "# Repair HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f242720c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "10\n",
      "11\n",
      "12\n",
      "14\n",
      "16\n",
      "17\n",
      "18\n",
      "2\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "3\n",
      "30\n",
      "35\n",
      "36\n",
      "37\n",
      "39\n",
      "4\n",
      "41\n",
      "42\n",
      "43\n",
      "45\n",
      "48\n",
      "49\n",
      "5\n",
      "6\n",
      "63\n",
      "64\n",
      "66\n",
      "67\n",
      "68\n",
      "7\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "76\n",
      "77\n",
      "81\n",
      "82\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "9\n",
      "90\n",
      "91\n",
      "92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "15\n",
      "20\n",
      "31\n",
      "32\n",
      "33\n",
      "40\n",
      "44\n",
      "46\n",
      "47\n",
      "65\n",
      "75\n",
      "79\n",
      "8\n",
      "83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "PATH = \"../data/80-20_calcium_dataset.h5\"\n",
    "with h5py.File(PATH, \"r+\") as file:\n",
    "    for fold, fold_values in tqdm(file.items(), leave=False):\n",
    "        for neuron, neuron_values in tqdm(fold_values.items(), leave=False):\n",
    "            print(neuron)\n",
    "            for cached in ['cache-input', 'cache-label', 'cache-unlabeled-input', 'cache-unlabeled-label']:\n",
    "                if cached in neuron_values:\n",
    "                    print(neuron_values.keys())\n",
    "#                     del neuron_values[cached]\n",
    "#             if \"cache-unlabeled-input\" in neuron_values:\n",
    "#                 print(neuron_values[\"cache-unlabeled-input\"].keys())\n",
    "#                 del neuron_values[\"cache-unlabeled-input\"]\n",
    "#             print(neuron_values.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569dc56a",
   "metadata": {},
   "source": [
    "# Extract small crops only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1623d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['complete-1:0', 'complete-1:8', 'complete-1:4', 'complete-1:2', 'complete-1:1'])\n",
      "{'neuron': '1', 'event-id': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ce965dfb38452195c40541d1a5565a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8eac2115614ee79c7c0117ae66caf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10f1741fd474f22be2a58b3d1c75ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2639 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38a4289484d4a84a3cfeb1f5919d5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy\n",
    "import h5py\n",
    "import random\n",
    "import copy\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "from skimage import filters\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MAX_PU = 8\n",
    "PATH = \"../data/80-20_calcium_dataset.h5\"\n",
    "numpy.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "SAMPLES_PU = json.load(open(\"../configs/training-samples_complete.json\", \"r\"))\n",
    "print(SAMPLES_PU.keys())\n",
    "\n",
    "print(SAMPLES_PU[\"complete-1:8\"][\"positive\"][\"train\"][0])\n",
    "\n",
    "DATASET = \"complete-1:8\"\n",
    "\n",
    "PATH = \"../data/80-20_calcium_dataset.h5\"\n",
    "OUTPATH = f\"../data/calcium_dataset_crops_{DATASET}_tmp.h5\"\n",
    "with h5py.File(OUTPATH, \"w\") as outfile:\n",
    "    with h5py.File(PATH, \"r\") as file:\n",
    "        for fold, fold_values in tqdm(file.items(), leave=False):\n",
    "            out_fold = outfile.create_group(fold)\n",
    "            for neuron, neuron_values in tqdm(fold_values.items(), leave=False):\n",
    "                out_neuron = out_fold.create_group(neuron)\n",
    "                out_neuron.create_group(\"input\")\n",
    "                out_neuron.create_group(\"label\")                \n",
    "\n",
    "            # Positive\n",
    "            for event in tqdm(SAMPLES_PU[DATASET][\"positive\"][fold]):\n",
    "                out_neuron = out_fold[str(event[\"neuron\"])]\n",
    "                out_neuron[\"input\"].create_dataset(str(event[\"event-id\"]), data=fold_values[str(event[\"neuron\"])][\"cache-input\"][str(event[\"event-id\"])], compression=\"gzip\", compression_opts=4)\n",
    "                out_neuron[\"label\"].create_dataset(str(event[\"event-id\"]), data=fold_values[str(event[\"neuron\"])][\"cache-label\"][str(event[\"event-id\"])], compression=\"gzip\", compression_opts=4, dtype=numpy.uint8)                \n",
    "                \n",
    "            # Negative\n",
    "            for event in tqdm(SAMPLES_PU[DATASET][\"negative\"][fold]):\n",
    "                out_neuron = out_fold[str(event[\"neuron\"])]\n",
    "                event_id = str(event[\"coord\"])\n",
    "                out_neuron[\"input\"].create_dataset(event_id, data=fold_values[str(event[\"neuron\"])][\"cache-unlabeled-input\"][event_id], compression=\"gzip\", compression_opts=4)\n",
    "                out_neuron[\"label\"].create_dataset(event_id, data=fold_values[str(event[\"neuron\"])][\"cache-unlabeled-label\"][event_id], compression=\"gzip\", compression_opts=4, dtype=numpy.uint8)                \n",
    "                    \n",
    "#                 for cached in ['cache-input', 'cache-label']:\n",
    "#                     out_cached = out_neuron.create_group(cached.split(\"cache-\")[-1])\n",
    "                    \n",
    "#                     for event_idx, event in neuron_values[cached].items():\n",
    "#                         out_cached.create_dataset(event_idx, data=event, compression=\"gzip\", compression_opts=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8ef242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPATH = f\"../data/calcium_dataset_crops_{DATASET}.h5\"\n",
    "with h5py.File(\"../data/database.hdf5\", \"w\") as file:\n",
    "    for fold in [\"train\", \"valid\"]:\n",
    "        fold_group = file.create_group(fold)\n",
    "        for neuron_id in range(5):\n",
    "            neuron_group = fold_group.create_group(str(neuron_id))\n",
    "            events = []\n",
    "            for event_id in range(10):\n",
    "                t, y, x = numpy.random.randint(10, size=3)\n",
    "                events.append([event_id, t, t + 64, y, y + 64, x, x + 64])\n",
    "\n",
    "            events = numpy.array(events, dtype=int)\n",
    "            input = numpy.random.rand(128, 96, 96).astype(numpy.float32)\n",
    "            label = (numpy.random.rand(128, 96, 96) > 0.9).astype(numpy.uint8)\n",
    "\n",
    "            neuron_group.create_dataset(\"events\", data=events)\n",
    "            neuron_group.create_dataset(\"input\", data=input)\n",
    "            neuron_group.create_dataset(\"label\", data=label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "44177add077f73cdf2e061317fce1df5378092b39d40de8164688d438f477cb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
