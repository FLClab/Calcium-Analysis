{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning on DCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using this notebook to show how to fine tune the network with new data\n",
    "\n",
    "**User inputs are capitalize throughout this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "sys.path.append(\"/home/gabriel/dev/automatic-transient-detection\")\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from skimage.external import tifffile\n",
    "\n",
    "# DataBase handler\n",
    "import database\n",
    "\n",
    "# Network handler\n",
    "from ATD.common.experiment import Experiment\n",
    "\n",
    "# Various utils functions\n",
    "from ATD.utils import ioutils as iou\n",
    "from ATD.utils import arrayutils as au\n",
    "from ATD.utils import calciumutils as cu\n",
    "from ATD.utils import listutils as lu\n",
    "from ATD.utils import datasetutils as du\n",
    "\n",
    "# because I am editing this file for now\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Creation\n",
    "The following section provides the code to create the databse from a [google spreadsheet](https://docs.google.com/spreadsheets/d/12tEG3494V3y24qU8f11Vzui85sqt3VTwUzHBfEl0sQA/edit#gid=409641959)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_DATABASE = True\n",
    "DATABASE_FILE = Path(\"../h5file/dcc_db.h5\") # location of the database\n",
    "SPREADSHEET = 'Raw Data List' # title of the spreadsheet document\n",
    "WORKSHEET = 'DCC' # title of the worksheet in the spreadsheet\n",
    "secretJSON = '../bot-reader-key.json' # file containing the authorizations to read the spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new database at ../h5file/dcc_db.h5...\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 0 on coverslip 0\n",
      "Experiment SEP-DCC with stream condition ctrl\n",
      "The stream Stream_GluA1_001.tif was image on 2018-06-18 00:00:00\n",
      "Unique stream-id is: stream-0\n",
      "A total of 1 streams was added...\n"
     ]
    }
   ],
   "source": [
    "# Get a worksheet object we will read with\n",
    "worksheet = database.Database.oauth_and_get_worksheet(SPREADSHEET, WORKSHEET, secretJSON)\n",
    "\n",
    "# Remove an existing database if it exists and CREATE_DATABASE is True\n",
    "if CREATE_DATABASE and DATABASE_FILE.exists():\n",
    "    DATABASE_FILE.unlink()\n",
    "    \n",
    "# Instanciate the database object and populate the databse from the worksheet\n",
    "db = database.Database(DATABASE_FILE)\n",
    "if CREATE_DATABASE:\n",
    "    db.load_streams_from_worksheet(worksheet)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference of the network on the movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will try on both of those network\n",
    "PRE_TRAINED_NETWORK_PATH = Path(\"/home/gabriel/results/atd/unet2d/unet2d-foldB-2fgr\") # the pre-trained network\n",
    "FINE_TUNED_NETWORK_PATH = Path(\"/home/gabriel/results/fine-tuned/sted/test/\") # the new fine-tuned network\n",
    "\n",
    "postprocess_params = {\n",
    "    'minimal_time': 1,\n",
    "    'minimal_height': 1,\n",
    "    'minimal_width': 1,\n",
    "    'threshold': 0.5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmentation and detection of 1 streams...\n",
      "========================\n",
      "Currently doing stream :\n",
      "Imaging of neuron 0 on coverslip 0\n",
      "Experiment SEP-DCC with stream condition ctrl\n",
      "The stream Stream_GluA1_001.tif was image on 2018-06-18 00:00:00\n",
      "Unique stream-id is: stream-0\n",
      "Stream shape: (1000, 512, 512)\n",
      "Infering on the stream...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:40<00:00,  4.54it/s]\n",
      "0 Regions removed so far: 100%|██████████| 539/539 [00:00<00:00, 1549.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 539 detected events, 0 removed\n"
     ]
    }
   ],
   "source": [
    "# Define the network experiment wrapper\n",
    "pretrained_xp = Experiment(str(PRE_TRAINED_NETWORK_PATH), save=False, verbose=False)\n",
    "finetuned_xp = Experiment(str(FINE_TUNED_NETWORK_PATH), save=False, verbose=False)\n",
    "\n",
    "# Load the database\n",
    "db = database.Database(DATABASE_FILE)\n",
    "\n",
    "# We will only infer on the first stream\n",
    "stream = db[0]\n",
    "\n",
    "db.segment_and_detect_msct(stream, finetuned_xp, postprocess_params)\n",
    "\n",
    "segmentation = stream['segmentation'][...]\n",
    "tifffile.imsave('/home/gabriel/results/fine-tuned/dcc/segmentation_finetuned_sted.tif', segmentation.astype(np.uint16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniFinder mSCTs Region Props Extraction\n",
    "Since we are using the MiniFinder analysis software (the mask location are listed in the column Mini-Finder in the worksheet), we need to extract the properties of the segmented regions from the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_MINI_REGION_PROPS = True\n",
    "REDO = True # if redo is True, if the properties already exist, they will be recalculated\n",
    "MINI_REGIONPROPS_PARAMS = {\n",
    "    'msct-props': [\n",
    "        'area', \n",
    "        'convex_area',\n",
    "        'bbox',\n",
    "        'max_intensity',\n",
    "        'major_axis_length',\n",
    "        'minor_axis_length'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting mini's properties of stream /streams/stream-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34 Regions removed so far: 100%|██████████| 323/323 [00:04<00:00, 79.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 289 detected events, 34 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23 Regions removed so far: 100%|██████████| 135/135 [00:01<00:00, 95.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 112 detected events, 23 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93 Regions removed so far: 100%|██████████| 736/736 [00:12<00:00, 60.05it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 643 detected events, 93 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24 Regions removed so far: 100%|██████████| 642/642 [00:03<00:00, 181.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 618 detected events, 24 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112 Regions removed so far: 100%|██████████| 708/708 [00:15<00:00, 46.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 596 detected events, 112 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31 Regions removed so far: 100%|██████████| 431/431 [00:04<00:00, 100.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 400 detected events, 31 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Regions removed so far: 100%|██████████| 396/396 [00:01<00:00, 238.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 385 detected events, 11 removed\n"
     ]
    }
   ],
   "source": [
    "# For this specific example, we dont have the mask of the last stream, \n",
    "# so we wont be extracting props of this stream\n",
    "db = database.Database(DATABASE_FILE)\n",
    "if EXTRACT_MINI_REGION_PROPS:\n",
    "    streams = list(db)\n",
    "    streams.pop(-1)\n",
    "    db.mini_regionprops_extraction(streams, MINI_REGIONPROPS_PARAMS, redo=REDO)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Network\n",
    "We will fine tune a pre-trained network and save it. \n",
    "We will also specified which stream we will be using to fine tune the network.\n",
    "We need to give the parameters of the method of extraction of Positive and Unlabeled and\n",
    "the parameters of the fine tuning itself. For the fine tuning parameters, any missing \n",
    "parameters are taken from the training configuration.\n",
    "**This step can take a while depending on the number of steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNE_NETWORK = False\n",
    "PRE_TRAINED_NETWORK_PATH = Path(\"/home/gabriel/results/atd/unet2d/unet2d-foldB-2fgr\") # the pre-trained network\n",
    "FINE_TUNED_NETWORK_PATH = Path(\"/home/gabriel/results/fine-tuned/sted/test/\") # the new fine-tuned network\n",
    "DRY_RUN = True # If dry_run is true, the fine-tuning wont save any checkpoints (for testing purposes)\n",
    "STREAM_CONDITION = 'sted'\n",
    "\n",
    "# P and U extraction parameters\n",
    "CROP_SIZE = 64\n",
    "FOREGROUND_RATIO = 0\n",
    "NUMBER_OF_POSITIVE = 500\n",
    "SHAPE2CROP = [5, 64, 64]\n",
    "PATTERN = [-2, -1, 0, 1, 2]\n",
    "\n",
    "# Fine-tuning parameters\n",
    "NUM_STEPS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = database.Database(DATABASE_FILE)\n",
    "if FINETUNE_NETWORK:\n",
    "    # Create the folder which will contain the fine tuned network\n",
    "    if FINE_TUNED_NETWORK_PATH.exists():\n",
    "        shutil.rmtree(FINE_TUNED_NETWORK_PATH)\n",
    "    shutil.copytree(PRE_TRAINED_NETWORK_PATH, FINE_TUNED_NETWORK_PATH)\n",
    "    if dry_run:\n",
    "        print(\"Dry run is set, the fine tuning wont be saved...\")\n",
    "\n",
    "    # Define the config dictionnary from the paramaters\n",
    "    configs = {\n",
    "        \"dataset_config\": {\n",
    "            \"number-of-positive\": NUMBER_OF_POSITIVE,\n",
    "            \"foreground-ratio\": FOREGROUND_RATIO,\n",
    "            \"crop-size\": CROP_SIZE,\n",
    "            \"pattern\": PATTERN\n",
    "        },\n",
    "        \"finetuning_config\": {\n",
    "            \"num_steps\": NUM_STEPS,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Define the network experiment wrapper\n",
    "    pretrained_xp = Experiment(str(PRE_TRAINED_NETWORK_PATH), save=False, verbose=False)\n",
    "    finetuned_xp = Experiment(str(FINE_TUNED_NETWORK_PATH), save=(not dry_run), verbose=False)\n",
    "    \n",
    "    # Define the fine tuning training streams\n",
    "    train_data = db.filter_by('stream-condition', 'train')\n",
    "\n",
    "    # Define the fine tuning validation streams\n",
    "    valid_data = db.filter_by('stream-condition', 'valid')\n",
    "    \n",
    "    # Do the actual fine tuning\n",
    "    db.fine_tune_network(train_data, valid_data, finetuned_xp, configs)\n",
    "    \n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Let's see how to fine-tuned network perform vs the pre-trained and Mini-Finder\n",
    "We will be using the stream with the 'stream-condition' 'to_test_fine_tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAM_CONDITION = 'to_test_fine_tuning'\n",
    "PRE_TRAINED_NETWORK_PATH = Path(\"/home/gabriel/results/atd/unet2d/unet2d-foldB-2fgr\") # the pre-trained network\n",
    "FINE_TUNED_NETWORK_PATH = Path(\"/home/gabriel/results/fine-tuned/sted/test/\") # the new fine-tuned network\n",
    "FINE_TUNE_FILENAME = 'mask_finetuned.tif'\n",
    "PRE_TRAINED_FILENAME = 'mask_pretrained.tif'\n",
    "MINI_FINDER_FILENAME = 'mask_minifinder.tif'\n",
    "STREAM_FILENAME = 'stream.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = database.Database(DATABASE_FILE)\n",
    "test_streams = db.filter_by('stream-condition', STREAM_CONDITION)\n",
    "assert len(test_streams) == 2 # There should be 2 streams\n",
    "with_mask, without_mask = test_streams\n",
    "\n",
    "# Define the network experiment wrapper\n",
    "pretrained_xp = Experiment(str(PRE_TRAINED_NETWORK_PATH), save=False, verbose=False)\n",
    "finetuned_xp = Experiment(str(FINE_TUNED_NETWORK_PATH), save=(not dry_run), verbose=False) \n",
    "\n",
    "# We will start by infering with the pretrained network and save the results\n",
    "db.segment_and_detect_msct(with_mask, pretrained_xp, redo=True)\n",
    "tifffile.imsave(PRE_TRAINED_FILENAME, with_mask['segmentation'][...].astype(np.int16))\n",
    "\n",
    "# We then infer with the finetuned network and save the results\n",
    "db.segment_and_detect_msct(with_mask, finetuned_xp, redo=True)\n",
    "tifffile.imsave(FINE_TUNE_FILENAME, with_mask['segmentation'][...].astype(np.int16))\n",
    "\n",
    "# We save the minifinder mask for convinience\n",
    "tifffile.imsave(MINI_FINDER_FILENAME, db.load_mini_mask(with_mask).astype(np.int16))\n",
    "\n",
    "# We also save the stream for convinience\n",
    "tifffile.imsave(STREAM_FILENAME, db.load_raw_stream(with_mask).astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atd",
   "language": "python",
   "name": "atd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}