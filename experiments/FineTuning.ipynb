{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using this notebook to show how to fine tune the network with new data\n",
    "\n",
    "**User inputs are capitalize throughout this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "sys.path.append(\"/home/gabriel/dev/automatic-transient-detection\")\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import collections\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# DataBase handler\n",
    "import database\n",
    "\n",
    "# Network handler\n",
    "from ATD.common.experiment import Experiment\n",
    "\n",
    "# Various utils functions\n",
    "from ATD.utils import ioutils as iou\n",
    "from ATD.utils import arrayutils as au\n",
    "from ATD.utils import calciumutils as cu\n",
    "from ATD.utils import listutils as lu\n",
    "from ATD.utils import datasetutils as du\n",
    "\n",
    "# because I am editing this file for now\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Creation\n",
    "The following section provides the code to create the databse from a [google spreadsheet](https://docs.google.com/spreadsheets/d/12tEG3494V3y24qU8f11Vzui85sqt3VTwUzHBfEl0sQA/edit#gid=409641959)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_DATABASE = True\n",
    "DATABASE_FILE = Path(\"../h5file/finetune_test2.h5\") # location of the database\n",
    "SPREADSHEET = 'Raw Data List' # title of the spreadsheet document\n",
    "WORKSHEET = 'FINE_TUNING_STED' # title of the worksheet in the spreadsheet\n",
    "secretJSON = '../bot-reader-key.json' # file containing the authorizations to read the spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new database at ../h5file/finetune_test2.h5...\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 635 on coverslip 0\n",
      "Experiment 100klam with stream condition train\n",
      "The stream Stream1_Cropped.tif was image on 2019-05-17 00:00:00\n",
      "Unique stream-id is: stream-0\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 636 on coverslip 0\n",
      "Experiment 100klam with stream condition train\n",
      "The stream Stream2_Cropped.tif was image on 2019-05-17 00:00:00\n",
      "Unique stream-id is: stream-1\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 576 on coverslip 3\n",
      "Experiment NMDA with stream condition train\n",
      "The stream Stream1_Aligned.tif was image on 2020-06-03 00:00:00\n",
      "Unique stream-id is: stream-2\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 576 on coverslip 3\n",
      "Experiment NMDA with stream condition train\n",
      "The stream Stream2_Aligned.tif was image on 2020-06-03 00:00:00\n",
      "Unique stream-id is: stream-3\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 577 on coverslip 4\n",
      "Experiment NMDA with stream condition valid\n",
      "The stream Stream1_Aligned.tif was image on 2020-06-03 00:00:00\n",
      "Unique stream-id is: stream-4\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 577 on coverslip 4\n",
      "Experiment NMDA with stream condition valid\n",
      "The stream Stream2_Aligned.tif was image on 2020-06-03 00:00:00\n",
      "Unique stream-id is: stream-5\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 578 on coverslip 5\n",
      "Experiment NMDA with stream condition valid\n",
      "The stream Stream2_Aligned.tif was image on 2020-06-03 00:00:00\n",
      "Unique stream-id is: stream-6\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 578 on coverslip 5\n",
      "Experiment NMDA with stream condition test_no_mask\n",
      "The stream Stream1_Aligned.tif was image on 2020-06-03 00:00:00\n",
      "Unique stream-id is: stream-7\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 551 on coverslip 0\n",
      "Experiment NMDA with stream condition train\n",
      "The stream Stream1_Aligned.tif was image on 2020-06-16 00:00:00\n",
      "Unique stream-id is: stream-8\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 551 on coverslip 0\n",
      "Experiment NMDA with stream condition train\n",
      "The stream Stream2_Aligned.tif was image on 2020-06-16 00:00:00\n",
      "Unique stream-id is: stream-9\n",
      "=================\n",
      "Adding the stream\n",
      "Imaging of neuron 551 on coverslip 0\n",
      "Experiment NMDA with stream condition train\n",
      "The stream Stream3_Aligned.tif was image on 2020-06-16 00:00:00\n",
      "Unique stream-id is: stream-10\n",
      "A total of 11 streams was added...\n"
     ]
    }
   ],
   "source": [
    "# Get a worksheet object we will read with\n",
    "worksheet = database.Database.oauth_and_get_worksheet(SPREADSHEET, WORKSHEET, secretJSON)\n",
    "\n",
    "# Remove an existing database if it exists and CREATE_DATABASE is True\n",
    "if CREATE_DATABASE and DATABASE_FILE.exists():\n",
    "    DATABASE_FILE.unlink()\n",
    "    \n",
    "# Instanciate the database object and populate the databse from the worksheet\n",
    "db = database.Database(DATABASE_FILE)\n",
    "if CREATE_DATABASE:\n",
    "    db.load_streams_from_worksheet(worksheet)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniFinder mSCTs Region Props Extraction\n",
    "Since we are using the MiniFinder analysis software (the mask location are listed in the column Mini-Finder in the worksheet), we need to extract the properties of the segmented regions from the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_MINI_REGION_PROPS = True\n",
    "REDO = True # if redo is True, if the properties already exist, they will be recalculated\n",
    "MINI_REGIONPROPS_PARAMS = {\n",
    "    'msct-props': [\n",
    "        'area', \n",
    "        'convex_area',\n",
    "        'bbox',\n",
    "        'max_intensity',\n",
    "        'major_axis_length',\n",
    "        'minor_axis_length'\n",
    "    ],\n",
    "    'minimal-shape': {\n",
    "        'minimal_time': 2,\n",
    "        'minimal_height': 3,\n",
    "        'minimal_width': 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting mini's properties of stream /streams/stream-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16 Regions removed so far: 100%|██████████| 323/323 [00:01<00:00, 163.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 307 detected events, 16 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9 Regions removed so far: 100%|██████████| 135/135 [00:00<00:00, 228.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 126 detected events, 9 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26 Regions removed so far: 100%|██████████| 736/736 [00:03<00:00, 192.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 710 detected events, 26 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24 Regions removed so far: 100%|██████████| 642/642 [00:03<00:00, 186.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 618 detected events, 24 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56 Regions removed so far: 100%|██████████| 708/708 [00:07<00:00, 92.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 652 detected events, 56 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31 Regions removed so far: 100%|██████████| 431/431 [00:04<00:00, 96.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 400 detected events, 31 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Regions removed so far: 100%|██████████| 396/396 [00:01<00:00, 240.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 385 detected events, 11 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27 Regions removed so far: 100%|██████████| 346/346 [00:03<00:00, 96.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 319 detected events, 27 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4 Regions removed so far: 100%|██████████| 54/54 [00:00<00:00, 98.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 50 detected events, 4 removed\n",
      "\n",
      "Extracting mini's properties of stream /streams/stream-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6 Regions removed so far: 100%|██████████| 94/94 [00:00<00:00, 115.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing centroids of events...\n",
      "There is 88 detected events, 6 removed\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# For this specific example, we dont have the mask of the last stream, \n",
    "# so we wont be extracting props of this stream\n",
    "db = database.Database(DATABASE_FILE)\n",
    "if EXTRACT_MINI_REGION_PROPS:\n",
    "    streams = list(db)\n",
    "    streams = [s for s in streams if s.attrs['stream-condition'] != 'test_no_mask']\n",
    "    db.mini_regionprops_extraction(streams, MINI_REGIONPROPS_PARAMS, redo=REDO)\n",
    "print('Done')\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tuning Network\n",
    "We will fine tune a pre-trained network and save it. \n",
    "We will also specified which stream we will be using to fine tune the network.\n",
    "We need to give the parameters of the method of extraction of Positive and Unlabeled and\n",
    "the parameters of the fine tuning itself. For the fine tuning parameters, any missing \n",
    "parameters are taken from the training configuration.\n",
    "**This step can take a while depending on the number of steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINETUNE_NETWORK = True\n",
    "PRE_TRAINED_NETWORK_PATH = Path(\"/home/gabriel/results/atd/unet2d/unet2d-foldB-2fgr\") # the pre-trained network\n",
    "FINE_TUNED_NETWORK_PATH = Path(\"/home/gabriel/results/fine-tuned/sted/test_2/\") # the new fine-tuned network\n",
    "DRY_RUN = False # If dry_run is true, the fine-tuning wont save any checkpoints (for testing purposes)\n",
    "\n",
    "# P and U extraction parameters\n",
    "CROP_SIZE = 64\n",
    "FOREGROUND_RATIO = 0\n",
    "NUMBER_OF_POSITIVE = 750\n",
    "SHAPE2CROP = [5, 64, 64]\n",
    "PATTERN = [-2, -1, 0, 1, 2]\n",
    "\n",
    "# Fine-tuning parameters\n",
    "NUM_STEPS = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning on 7 training streams...\n",
      "Fine tuning on 3 validation streams...\n",
      "There is 307 events in stream <HDF5 group \"/streams/stream-0\" (11 members)>\n",
      "There is 126 events in stream <HDF5 group \"/streams/stream-1\" (11 members)>\n",
      "There is 710 events in stream <HDF5 group \"/streams/stream-2\" (11 members)>\n",
      "There is 618 events in stream <HDF5 group \"/streams/stream-3\" (11 members)>\n",
      "There is 319 events in stream <HDF5 group \"/streams/stream-8\" (11 members)>\n",
      "There is 50 events in stream <HDF5 group \"/streams/stream-9\" (11 members)>\n",
      "There is 88 events in stream <HDF5 group \"/streams/stream-10\" (11 members)>\n",
      "750/2218 mSCTs will be extracted...\n",
      "0 foreground will be extracted...\n",
      "Loading stream 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 70/750 [00:00<00:00, 696.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stream 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 94/750 [00:04<00:39, 16.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stream 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 347/750 [00:19<00:20, 19.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stream 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 572/750 [00:35<00:06, 25.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stream 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 665/750 [00:50<00:11,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stream 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▍| 709/750 [01:05<00:08,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stream 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [01:21<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning on 750 training crops...\n",
      "There is 652 events in stream <HDF5 group \"/streams/stream-4\" (11 members)>\n",
      "There is 400 events in stream <HDF5 group \"/streams/stream-5\" (11 members)>\n",
      "There is 385 events in stream <HDF5 group \"/streams/stream-6\" (11 members)>\n",
      "1437/1437 mSCTs will be extracted...\n",
      "0 foreground will be extracted...\n",
      "Loading stream 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 636/1437 [00:00<00:01, 710.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stream 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1012/1437 [00:16<00:03, 135.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stream 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1437/1437 [00:32<00:00, 44.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning on 1437 validation crops...\n",
      "\u001b[32m\u001b[1m [-] Info: The model has 1777537 parameters\u001b[0m\n",
      "\u001b[32m\u001b[1m [-] Info: Training for 10000 steps\u001b[0m\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1723cc7092bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Do the actual fine tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine_tune_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinetuned_xp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/automatic-transient-detection/database.py\u001b[0m in \u001b[0;36mfine_tune_network\u001b[0;34m(self, train_data, valid_data, network_xp, configs)\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0mLOG_INTERVAL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m         \u001b[0mSAVE_INTERVAL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1597\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOG_INTERVAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAVE_INTERVAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompare_with_MiniFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/automatic-transient-detection/ATD/models/unet2d/trainer.py\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(self, train, valid, log_interval, save_interval, device)\u001b[0m\n\u001b[1;32m    443\u001b[0m             self.results_procedure(model, valid_loader, optimizer, loss_dict,\n\u001b[1;32m    444\u001b[0m                                    \u001b[0mvalid_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                                    device=device)\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m#self.log_procedure(model, test_loader, optimizer, loss_dict,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/automatic-transient-detection/ATD/models/unet2d/trainer.py\u001b[0m in \u001b[0;36mresults_procedure\u001b[0;34m(self, model, valid_loader, optimizer, loss_dict, valid_dict, duration, valid_interval, device)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalid_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0mvalid_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid-mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/automatic-transient-detection/ATD/metrics.py\u001b[0m in \u001b[0;36mcompute_MSE\u001b[0;34m(model, loader, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mall_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atd/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atd/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atd/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atd/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atd/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atd/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/atd/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "db = database.Database(DATABASE_FILE)\n",
    "if FINETUNE_NETWORK:\n",
    "    # Create the folder which will contain the fine tuned network\n",
    "    if FINE_TUNED_NETWORK_PATH.exists():\n",
    "        shutil.rmtree(FINE_TUNED_NETWORK_PATH)\n",
    "    shutil.copytree(PRE_TRAINED_NETWORK_PATH, FINE_TUNED_NETWORK_PATH)\n",
    "    if DRY_RUN:\n",
    "        print(\"Dry run is set, the fine tuning wont be saved...\")\n",
    "\n",
    "    # Define the config dictionnary from the paramaters\n",
    "    configs = {\n",
    "        \"dataset_config\": {\n",
    "            \"number-of-positive\": NUMBER_OF_POSITIVE,\n",
    "            \"foreground-ratio\": FOREGROUND_RATIO,\n",
    "            \"crop-size\": CROP_SIZE,\n",
    "            \"pattern\": PATTERN\n",
    "        },\n",
    "        \"finetuning_config\": {\n",
    "            \"num_steps\": NUM_STEPS,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Define the network experiment wrapper\n",
    "    pretrained_xp = Experiment(str(PRE_TRAINED_NETWORK_PATH), save=False, verbose=False)\n",
    "    finetuned_xp = Experiment(str(FINE_TUNED_NETWORK_PATH), save=(not DRY_RUN), verbose=False)\n",
    "    \n",
    "    # Define the fine tuning training streams\n",
    "    train_data = db.filter_by('stream-condition', 'train')\n",
    "\n",
    "    # Define the fine tuning validation streams\n",
    "    valid_data = db.filter_by('stream-condition', 'valid')\n",
    "    \n",
    "    # Do the actual fine tuning\n",
    "    db.fine_tune_network(train_data, valid_data, finetuned_xp, configs)\n",
    "    \n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Let's see how to fine-tuned network perform vs the pre-trained and Mini-Finder\n",
    "We will be using the stream with the 'stream-condition' 'to_test_fine_tuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "STREAM_CONDITION = 'to_test_fine_tuning'\n",
    "PRE_TRAINED_NETWORK_PATH = Path(\"/home/gabriel/results/atd/unet2d/unet2d-foldB-2fgr\") # the pre-trained network\n",
    "FINE_TUNED_NETWORK_PATH = Path(\"/home/gabriel/results/fine-tuned/sted/test/\") # the new fine-tuned network\n",
    "FINE_TUNE_FILENAME = 'mask_finetuned.tif'\n",
    "PRE_TRAINED_FILENAME = 'mask_pretrained.tif'\n",
    "MINI_FINDER_FILENAME = 'mask_minifinder.tif'\n",
    "STREAM_FILENAME = 'stream.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = database.Database(DATABASE_FILE)\n",
    "test_streams = db.filter_by('stream-condition', STREAM_CONDITION)\n",
    "assert len(test_streams) == 2 # There should be 2 streams\n",
    "with_mask, without_mask = test_streams\n",
    "\n",
    "# Define the network experiment wrapper\n",
    "pretrained_xp = Experiment(str(PRE_TRAINED_NETWORK_PATH), save=False, verbose=False)\n",
    "finetuned_xp = Experiment(str(FINE_TUNED_NETWORK_PATH), save=(not dry_run), verbose=False) \n",
    "\n",
    "# We will start by infering with the pretrained network and save the results\n",
    "db.segment_and_detect_msct(with_mask, pretrained_xp, redo=True)\n",
    "tifffile.imsave(PRE_TRAINED_FILENAME, with_mask['segmentation'][...].astype(np.int16))\n",
    "\n",
    "# We then infer with the finetuned network and save the results\n",
    "db.segment_and_detect_msct(with_mask, finetuned_xp, redo=True)\n",
    "tifffile.imsave(FINE_TUNE_FILENAME, with_mask['segmentation'][...].astype(np.int16))\n",
    "\n",
    "# We save the minifinder mask for convinience\n",
    "tifffile.imsave(MINI_FINDER_FILENAME, db.load_mini_mask(with_mask).astype(np.int16))\n",
    "\n",
    "# We also save the stream for convinience\n",
    "tifffile.imsave(STREAM_FILENAME, db.load_raw_stream(with_mask).astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atd",
   "language": "python",
   "name": "atd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}