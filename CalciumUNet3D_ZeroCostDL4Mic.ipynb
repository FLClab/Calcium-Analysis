{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av1qDcfthk1a"
      },
      "source": [
        "# **Calcium U-Net 3D**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4> Description of the network and link to publication with author reference. [Beaupré et al, etc.](URL).\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>*Disclaimer*:\n",
        "\n",
        "<font size = 4>This notebook is inspired from the *Zero-Cost Deep-Learning to Enhance Microscopy* project (ZeroCostDL4Mic) (https://github.com/HenriquesLab/DeepLearning_Collab/wiki) and was created by **Frédéric Beaupré and Anthony Bilodeau**\n",
        "\n",
        "<font size = 4>This notebook is based on the following paper:\n",
        "\n",
        "<font size = 4>**Quantitative Analyis of Miniature Synaptic Calcium Transients Using Positive Unlabeled Deep Learning**, Journal, volume, pages, year and Frédéric Beaupré, Anthony Bilodeau, Theresa Wiesner, Gabriel Leclerc, Mado Lemieux, Gabriel Nadeau, Katrine Castonguay, BoLin Fan, Simon Labrecque, Renée Hložek, Paul De Koninck, Christian Gagné, Flavie Lavoie-Cardinal, [link to paper](URL)\n",
        "\n",
        "<font size = 4>And source code found in: [Github](https://github.com/FLClab/Calcium-Analysis)\n",
        "\n",
        "<font size = 4>The dataset and models are available in: [link](https://s3.valeria.science/flclab-calcium/index.html)\n",
        "\n",
        "\n",
        "<font size = 4>**Please also cite this original paper when using or developing this notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKktwSaWhq9e"
      },
      "source": [
        "# **How to use this notebook?**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>Video describing how to use ZeroCostDL4Mic notebooks are available on youtube:\n",
        "  - [**Video 1**](https://www.youtube.com/watch?v=GzD2gamVNHI&feature=youtu.be): Full run through of the workflow to obtain the notebooks and the provided test datasets as well as a common use of the notebook\n",
        "  - [**Video 2**](https://www.youtube.com/watch?v=PUuQfP5SsqM&feature=youtu.be): Detailed description of the different sections of the notebook\n",
        "\n",
        "\n",
        "---\n",
        "### **Structure of a notebook**\n",
        "\n",
        "<font size = 4>The notebook contains two types of cell:  \n",
        "\n",
        "<font size = 4>**Text cells** provide information and can be modified by douple-clicking the cell. You are currently reading the text cell. You can create a new text by clicking `+ Text`.\n",
        "\n",
        "<font size = 4>**Code cells** contain code and the code can be modfied by selecting the cell. To execute the cell, move your cursor on the `[ ]`-mark on the left side of the cell (play button appears). Click to execute the cell. After execution is done the animation of play button stops. You can create a new coding cell by clicking `+ Code`.\n",
        "\n",
        "---\n",
        "### **Table of contents, Code snippets** and **Files**\n",
        "\n",
        "<font size = 4>On the top left side of the notebook you find three tabs which contain from top to bottom:\n",
        "\n",
        "<font size = 4>*Table of contents* = contains structure of the notebook. Click the content to move quickly between sections.\n",
        "\n",
        "<font size = 4>*Code snippets* = contain examples how to code certain tasks. You can ignore this when using this notebook.\n",
        "\n",
        "<font size = 4>*Files* = contain all available files. After mounting your google drive (see section 1.) you will find your files and folders here.\n",
        "\n",
        "<font size = 4>**Remember that all uploaded files are purged after changing the runtime.** All files saved in Google Drive will remain. You do not need to use the Mount Drive-button; your Google Drive is connected in section 1.2.\n",
        "\n",
        "<font size = 4>**Note:** The \"sample data\" in \"Files\" contains default files. Do not upload anything in here!\n",
        "\n",
        "---\n",
        "### **Making changes to the notebook**\n",
        "\n",
        "<font size = 4>**You can make a copy** of the notebook and save it to your Google Drive. To do this click file -> save a copy in drive.\n",
        "\n",
        "<font size = 4>To **edit a cell**, double click on the text. This will show you either the source code (in code cells) or the source text (in text cells).\n",
        "You can use the `#`-mark in code cells to comment out parts of the code. This allows you to keep the original code piece in the cell as a comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v_Jl2QZhvLh"
      },
      "source": [
        "# **0. Before getting started**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>**We strongly recommend that you generate extra paired images. These images can be used to assess the quality of your trained model (Quality control dataset)**. The quality control assessment can be done directly in this notebook.\n",
        "\n",
        "<font size = 4> **Additionally, the corresponding input and output files need to have the same name**.\n",
        "\n",
        "<font size = 4> Please note that you currently can **only use .tif files!**\n",
        "\n",
        "---\n",
        "    \n",
        "### **Directory Structure**\n",
        "   \n",
        "<font size = 4>Make sure to have the current structure for training data. We will use *\\<example\\>* to show that the naming convention is left to the user and is not specific. In all cases, it is assumed that the movies were already normalized.\n",
        "\n",
        "<font size = 4>**Structure 1:** HDF5 file (versatile)\n",
        "\n",
        "This file structure is the most versatile approach. It allows to load events from movies.\n",
        "    \n",
        "```\n",
        "<file.hdf5>\n",
        "|---train\n",
        "|   |---<stream-id-0>\n",
        "|   |   |---input: [N, H, W] (dtype: float32)\n",
        "|   |   |---label: [N, H, W] (dtype: unit16)\n",
        "|   |   |---events: [M, 7] (event-id, *bbox_coords)\n",
        "|   |---<stream-id-1>\n",
        "|   |   |---...\n",
        "|   |---...\n",
        "|---valid\n",
        "|   |---<stream-id-0>\n",
        "|   |   |---input: [N, H, W] (dtype: float32)\n",
        "|   |   |---label: [N, H, W] (dtype: unit16)\n",
        "|   |   |---events: [M, 7] (event-id, *bbox_coords)\n",
        "|   |---<stream-id-1>\n",
        "|   |   |---...\n",
        "|   |---...    \n",
        "```\n",
        "\n",
        "**OR**, the events could already have been extracted from each streams using this\n",
        "    \n",
        "```\n",
        "<file.hdf5>\n",
        "|---train\n",
        "|   |---<stream-id-0>\n",
        "|   |   |---input:\n",
        "|   |   |   |---<event-id-0>: [N, H, W] (dtype: float32)\n",
        "|   |   |   |---<event-id-1>: [N, H, W] (dtype: float32)     \n",
        "|   |   |   |---...     \n",
        "|   |   |---label:\n",
        "|   |   |   |---<event-id-0>: [N, H, W] (dtype: uint16)\n",
        "|   |   |   |---<event-id-1>: [N, H, W] (dtype: uint16)     \n",
        "|   |   |   |---...    \n",
        "|   |---<stream-id-1>\n",
        "|   |   |---...\n",
        "|   |---...\n",
        "|---valid\n",
        "|   |---<stream-id-0>\n",
        "|   |   |---input:\n",
        "|   |   |   |---<event-id-0>: [N, H, W] (dtype: float32)\n",
        "|   |   |   |---<event-id-1>: [N, H, W] (dtype: float32)     \n",
        "|   |   |   |---...     \n",
        "|   |   |---label:\n",
        "|   |   |   |---<event-id-0>: [N, H, W] (dtype: uint16)\n",
        "|   |   |   |---<event-id-1>: [N, H, W] (dtype: uint16)     \n",
        "|   |   |   |---...    \n",
        "|   |---<stream-id-1>\n",
        "|   |   |---...\n",
        "|   |---...    \n",
        "```    \n",
        "    \n",
        "<font size = 4>**Structure 2:** Tiff files\n",
        "    \n",
        "<font size = 4>When using tiff files, it is assumed that center crops around events have already been extracted. The training and validation folds will be automatically created when using this structure. The filenames in the folder containing the streams and the folder containing the labels should match.\n",
        "```\n",
        "<path/to/stream-folder>\n",
        "|---<movie0>.tif\n",
        "|---<movie1>.tif\n",
        "|---...    \n",
        "<path/to/target-folder>\n",
        "|---<movie0>.tif\n",
        "|---<movie1>.tif\n",
        "|---...\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "    \n",
        "<font size = 4>**Important note**\n",
        "\n",
        "<font size = 4>- If you wish to **Train a network from scratch** using your own dataset (and we encourage everyone to do that), you will need to run **sections 1 - 4**, then use **section 5** to assess the quality of your model and **section 6** to run predictions using the model that you trained.\n",
        "\n",
        "<font size = 4>- If you wish to **Evaluate your model** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 5** to assess the quality of your model.\n",
        "\n",
        "<font size = 4>- If you only wish to **run predictions** using a model previously generated and saved on your Google Drive, you will only need to run **sections 1 and 2** to set up the notebook, then use **section 6** to run the predictions on the desired model.\n",
        "    \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvJvtQQgiVDF"
      },
      "source": [
        "# **1. Install Calcium U-Net 3D and dependencies**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bErTS2s-Mg2e"
      },
      "source": [
        "## **1.1. Install key dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qEl-IDUFMg2e"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play to install Calcium U-Net 3D dependencies\n",
        "# Install packages which are not included in Google Colab\n",
        "\n",
        "!pip install -q wget\n",
        "!pip install -q tifffile\n",
        "!pip install -q fpdf2\n",
        "!pip install -q git+https://github.com/FLClab/metrics.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOqQdGV5Mg2g"
      },
      "source": [
        "## **1.2. Restart your runtime**\n",
        "---\n",
        "<font size = 4>\n",
        "\n",
        "\n",
        "**<font size = 4> Ignore the following message error message. Your Runtime has automatically restarted. This is normal.**\n",
        "\n",
        "<img width=\"40%\" alt =\"\" src=\"https://github.com/HenriquesLab/ZeroCostDL4Mic/raw/master/Wiki_files/session_crash.png\"><figcaption>  </figcaption>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5UasJiMMg2g"
      },
      "source": [
        "## **1.3. Load key dependencies**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XMi71QrxiZbS"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Install Network and dependencies\n",
        "\n",
        "#Libraries contains information of certain topics.\n",
        "\n",
        "#Put the imported code and libraries here\n",
        "\n",
        "Notebook_version = ['1.12'] #Contact the ZeroCostDL4Mic team to find out about the version number\n",
        "Network = \"Calcium U-Net 3D\"\n",
        "\n",
        "from builtins import any as b_any\n",
        "\n",
        "def get_requirements_path():\n",
        "    # Store requirements file in 'contents' directory\n",
        "    current_dir = os.getcwd()\n",
        "    dir_count = current_dir.count('/') - 1\n",
        "    path = '../' * (dir_count) + 'requirements.txt'\n",
        "    return path\n",
        "\n",
        "def filter_files(file_list, filter_list):\n",
        "    filtered_list = []\n",
        "    for fname in file_list:\n",
        "        if b_any(fname.split('==')[0] in s for s in filter_list):\n",
        "            filtered_list.append(fname)\n",
        "    return filtered_list\n",
        "\n",
        "def build_requirements_file(before, after):\n",
        "    path = get_requirements_path()\n",
        "\n",
        "    # Exporting requirements.txt for local run\n",
        "    !pip freeze > $path\n",
        "\n",
        "    # Get minimum requirements file\n",
        "    df = pandas.read_csv(path, delimiter = \"\\n\")\n",
        "    mod_list = [m.split('.')[0] for m in after if not m in before]\n",
        "    req_list_temp = df.values.tolist()\n",
        "    req_list = [x[0] for x in req_list_temp]\n",
        "\n",
        "    # Replace with package name and handle cases where import name is different to module name\n",
        "    mod_name_list = [['sklearn', 'scikit-learn'], ['skimage', 'scikit-image']]\n",
        "    mod_replace_list = [[x[1] for x in mod_name_list] if s in [x[0] for x in mod_name_list] else s for s in mod_list]\n",
        "    filtered_list = filter_files(req_list, mod_replace_list)\n",
        "\n",
        "    file=open(path,'w')\n",
        "    for item in filtered_list:\n",
        "        file.writelines(item + '\\n')\n",
        "\n",
        "    file.close()\n",
        "\n",
        "import sys\n",
        "before = [str(m) for m in sys.modules]\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import wget\n",
        "import itertools\n",
        "import numpy\n",
        "import pandas\n",
        "import shutil\n",
        "import h5py\n",
        "import random\n",
        "import tifffile\n",
        "import yaml\n",
        "import scipy\n",
        "import json\n",
        "import csv\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "from torchvision.transforms import Compose\n",
        "from collections import defaultdict\n",
        "from matplotlib import pyplot\n",
        "from skimage import filters, io, measure\n",
        "from sklearn.metrics import precision_recall_curve, precision_score, recall_score, jaccard_score, f1_score\n",
        "from scipy.spatial import distance\n",
        "from metrics import CentroidDetectionError\n",
        "\n",
        "import time\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "from fpdf import FPDF, HTMLMixin\n",
        "from pip._internal.operations.freeze import freeze\n",
        "\n",
        "from ipywidgets import interact\n",
        "from ipywidgets import interactive\n",
        "from ipywidgets import fixed\n",
        "from ipywidgets import interact_manual\n",
        "import ipywidgets as widgets\n",
        "\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def bar_progress(current, total, width=80):\n",
        "  progress_message = \"Downloading: %d%% [%d / %d] bytes\" % (current / total * 100, current, total)\n",
        "  # Don't use print() as it will print in new line every time.\n",
        "  sys.stdout.write(\"\\r\" + progress_message)\n",
        "  sys.stdout.flush()\n",
        "\n",
        "################################################################################################\n",
        "# MODEL DEFINITION\n",
        "################################################################################################\n",
        "\n",
        "class DownConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Module pour faire 2 convolutions et un max pooling.\n",
        "    ReLU ou LReLU et BatchNorm apres chaque convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inC, outC, kernel_size=3, pooling=True,\n",
        "                 use_leaky_relu=False, use_batch_norm=False):\n",
        "        super().__init__()\n",
        "        self.inC = inC\n",
        "        self.outC = outC\n",
        "        self.pooling = pooling\n",
        "        self.use_leaky_relu = use_leaky_relu\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        if use_leaky_relu:\n",
        "            relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        else:\n",
        "            relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.convs = []\n",
        "        self.convs.append(nn.Conv3d(inC, outC, kernel_size=kernel_size, padding=kernel_size // 2))\n",
        "        if use_batch_norm:\n",
        "            self.convs.append(nn.BatchNorm3d(outC))\n",
        "        self.convs.append(relu)\n",
        "        self.convs.append(nn.Conv3d(outC, outC, kernel_size=kernel_size, padding=kernel_size // 2))\n",
        "        if use_batch_norm:\n",
        "            self.convs.append(nn.BatchNorm3d(outC))\n",
        "        self.convs.append(relu)\n",
        "        self.convs = nn.ModuleList(self.convs)\n",
        "\n",
        "        if pooling:\n",
        "            self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, module in enumerate(self.convs):\n",
        "            x = module(x)\n",
        "        before_pool = x\n",
        "        if self.pooling:\n",
        "            x = self.pool(x)\n",
        "        return x, before_pool\n",
        "\n",
        "class UpConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Module pour faire une upconv et 2 convolutions.\n",
        "    ReLU et BatchNorm apres chaque convolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inC, outC, use_batch_norm=False):\n",
        "        super().__init__()\n",
        "        self.inC = inC\n",
        "        self.outC = outC\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.upconv = nn.ConvTranspose3d(inC, outC, kernel_size=3,\n",
        "                                         stride=2, output_padding=1,\n",
        "                                         padding=1)\n",
        "\n",
        "        self.convs = []\n",
        "        self.convs.append(nn.Conv3d(inC, outC, kernel_size=3, padding=1))\n",
        "        if self.use_batch_norm:\n",
        "            self.convs.append(nn.BatchNorm3d(outC))\n",
        "        self.convs.append(relu)\n",
        "        self.convs.append(nn.Conv3d(outC, outC, kernel_size=3, padding=1))\n",
        "        if self.use_batch_norm:\n",
        "            self.convs.append(nn.BatchNorm3d(outC))\n",
        "        self.convs.append(relu)\n",
        "\n",
        "        self.convs = nn.ModuleList(self.convs)\n",
        "\n",
        "    def forward(self, from_enco, from_deco):\n",
        "        from_deco = self.upconv(from_deco)\n",
        "        x = torch.cat([from_enco, from_deco], 1)\n",
        "        for module in self.convs:\n",
        "            x = module(x)\n",
        "        return x\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    \"\"\"\n",
        "    Module pour faire les opérations dans le bas\n",
        "    du réseau (le bottleneck). Il y a une conv,\n",
        "    des convolutions atrous (concatened together)\n",
        "    suivi d'une conv. Avec r=1, il s'agit d'une\n",
        "    conv normale.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inC, outC, r=1):\n",
        "        super().__init__()\n",
        "        self.inC = inC\n",
        "        self.outC = outC\n",
        "        self.r = r\n",
        "\n",
        "        self.dilated_conv = []\n",
        "        for i in range(r):\n",
        "            dilation_conv = nn.Conv3d(inC, outC, kernel_size=3,\n",
        "                                      padding=i + 1, dilation=i + 1)\n",
        "            self.dilated_conv.append(dilation_conv)\n",
        "        self.dilated_conv = nn.ModuleList(self.dilated_conv)\n",
        "        self.conv2 = nn.Conv3d(outC * r, outC, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = [F.relu(d_conv(x)) for d_conv in self.dilated_conv]\n",
        "        x = torch.cat(x, 1)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        return x\n",
        "\n",
        "class UNet3D(nn.Module):\n",
        "    \"\"\"\n",
        "    Implémentation de http://www.nature.com/articles/s41598-018-34817-6\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        \"\"\"Defines the UNet network.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "        model_config = self.config[\"model_config\"]\n",
        "\n",
        "        self.first_kernel_size = model_config[\"first_kernel_size\"]\n",
        "        self.n_layer = model_config[\"n_layer\"]\n",
        "        self.nbf = model_config[\"nbf\"]\n",
        "        self.inC = model_config[\"inC\"]\n",
        "        self.n_layer = model_config[\"n_layer\"]\n",
        "        self.nbf = model_config[\"nbf\"]\n",
        "        self.outC = model_config[\"outC\"]\n",
        "        self.r = model_config[\"r\"]\n",
        "        self.use_batch_norm = model_config[\"use_batch_norm\"]\n",
        "        self.use_leaky_relu = model_config[\"use_leaky_relu\"]\n",
        "\n",
        "        self.down_convs = []\n",
        "        self.up_convs = []\n",
        "\n",
        "        for i in range(self.n_layer):\n",
        "            ins = self.inC if i == 0 else outs\n",
        "            outs = self.nbf * (2**i)\n",
        "            if i == 0:  # first conv\n",
        "                down_conv = DownConv(ins, outs, kernel_size=self.first_kernel_size,\n",
        "                                     use_leaky_relu=self.use_leaky_relu, use_batch_norm=self.use_batch_norm)\n",
        "            else:\n",
        "                down_conv = DownConv(ins, outs,\n",
        "                                     use_leaky_relu=self.use_leaky_relu, use_batch_norm=self.use_batch_norm)\n",
        "            self.down_convs.append(down_conv)\n",
        "\n",
        "        ins = outs\n",
        "        outs = self.nbf * (2**(i + 1))\n",
        "        self.bottleneck = BottleNeck(ins, outs, r=self.r)\n",
        "\n",
        "        for i in range(self.n_layer):\n",
        "            ins = outs\n",
        "            outs = ins // 2\n",
        "            up_conv = UpConv(ins, outs, use_batch_norm=self.use_batch_norm)\n",
        "            self.up_convs.append(up_conv)\n",
        "\n",
        "        self.down_convs = nn.ModuleList(self.down_convs)\n",
        "        self.up_convs = nn.ModuleList(self.up_convs)\n",
        "        self.final_conv = nn.Conv3d(outs, self.outC, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outs = []\n",
        "        for i, module in enumerate(self.down_convs):\n",
        "            x, before_pool = module(x)\n",
        "            encoder_outs.append(before_pool)\n",
        "\n",
        "        x = self.bottleneck(x)\n",
        "        for i, module in enumerate(self.up_convs):\n",
        "            before_pool = encoder_outs[-(i + 1)]\n",
        "            x = module(before_pool, x)\n",
        "        x = self.final_conv(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "    def predict(self, x):\n",
        "        with torch.no_grad():\n",
        "            out = self.forward(x)\n",
        "        return out\n",
        "\n",
        "    def predict_stream(self, streamNPZ, step=None, batch_size=1, num_workers=1, device='cpu'):\n",
        "        \"\"\"Fonction pour effectuer une prédiction sur un stream.\n",
        "\n",
        "        Inputs:\n",
        "            streamNPZ (NpzFile): Fichier Npz contenant le nécessaire pour faire la prédiction.\n",
        "            step (list of int): Défini les step d'évaluation dans les 3 axes du stream\n",
        "            batch_size (int): La batch size a utiliser pour inférer.\n",
        "        \"\"\"\n",
        "        stream = streamNPZ['input']\n",
        "        shape2crop = streamNPZ['shape2crop']\n",
        "        stream = numpy.pad(\n",
        "            stream, tuple((s, s) for s in shape2crop), mode=\"symmetric\"\n",
        "        )\n",
        "        stream_ds = PredictStream3D(stream, shape2crop, step=step)\n",
        "        stream_dl = DataLoader(stream_ds, batch_size=batch_size,\n",
        "                               shuffle=False, num_workers=num_workers)\n",
        "\n",
        "        # Matrices pour sauvegarder les prédictions\n",
        "        nbPixel = numpy.zeros_like(stream, dtype=numpy.uint8)\n",
        "        probAccumulator = numpy.zeros_like(stream, dtype=numpy.float32)\n",
        "\n",
        "        for batch_of_crops, batch_of_corners in tqdm(stream_dl, desc=\"Prediction: \"):\n",
        "            batch_of_crops = batch_of_crops.to(device)\n",
        "            batch_of_corners = batch_of_corners.numpy()\n",
        "            preds = self.predict(batch_of_crops)\n",
        "            preds = preds.cpu().numpy()\n",
        "            for b in range(batch_of_crops.shape[0]):\n",
        "                corner = batch_of_corners[b]\n",
        "                slicesTHW = (slice(corner[0], corner[0] + shape2crop[0]),\n",
        "                             slice(corner[1], corner[1] + shape2crop[1]),\n",
        "                             slice(corner[2], corner[2] + shape2crop[2]))\n",
        "                probAccumulator[slicesTHW] += preds[b, 0]\n",
        "                nbPixel[slicesTHW] += 1\n",
        "\n",
        "            del batch_of_crops, preds\n",
        "\n",
        "        slc = tuple(slice(s, -s) for s in shape2crop)\n",
        "        probAccumulator = probAccumulator[slc]\n",
        "        nbPixel = nbPixel[slc]\n",
        "        return probAccumulator / nbPixel\n",
        "\n",
        "    def optimize_threshold(self,\n",
        "                          valid_dataset):\n",
        "\n",
        "        training_config = self.config[\"training_config\"]\n",
        "        valid_loader = DataLoader(valid_dataset, batch_size=training_config[\"batch_size\"],\n",
        "                                num_workers=training_config[\"num_workers\"], shuffle=True,\n",
        "                                drop_last=training_config[\"drop_last\"], pin_memory=True)\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        thresholds = []\n",
        "        for batch in tqdm(valid_loader, desc=\"Validation Loader: \", leave=False):\n",
        "\n",
        "            event, mask, index = [t.to(DEVICE) for t in batch]\n",
        "\n",
        "            # defines x and y\n",
        "            x = event\n",
        "            y = mask\n",
        "\n",
        "            pred = self.forward(x)\n",
        "\n",
        "            thresholds.extend(compute_optimal_thresholds(mask, pred))\n",
        "\n",
        "        threshold = numpy.mean(thresholds)\n",
        "\n",
        "        return threshold\n",
        "\n",
        "    def optimize_threshold_detection(self,\n",
        "                          valid_dataset,\n",
        "                          **kwargs):\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        patch_size = kwargs.pop(\"patch_size\", 32)\n",
        "\n",
        "        thresholds = []\n",
        "        for event, mask, index in tqdm(valid_dataset, desc=\"Validation Dataset: \", leave=False):\n",
        "            event, mask = event.squeeze(), mask.squeeze()\n",
        "            pred = self.predict_stream({\n",
        "                \"input\" : event,\n",
        "                \"shape2crop\" : numpy.array([patch_size * 2, patch_size*2, patch_size*2])\n",
        "                },\n",
        "                **kwargs\n",
        "            )\n",
        "\n",
        "            thresholds.append(compute_optimal_thresholds_detection(mask, pred))\n",
        "\n",
        "        threshold = numpy.mean(thresholds)\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        return threshold\n",
        "\n",
        "    def train_model(self,\n",
        "              train_dataset,\n",
        "              valid_dataset,\n",
        "              model_path,\n",
        "              model_name,\n",
        "              ckpt_period=1,\n",
        "              save_best_ckpt_only=False,\n",
        "              ckpt_path=None,\n",
        "              resume_training=True):\n",
        "\n",
        "        # Create quality control folder\n",
        "        quality_control_path = os.path.join(model_path, model_name, \"Quality Control\")\n",
        "        if not os.path.isdir(quality_control_path):\n",
        "            os.makedirs(quality_control_path, exist_ok=True)\n",
        "\n",
        "        # Export configuration file\n",
        "        yaml.dump(self.config, open(os.path.join(model_path, model_name, \"config.yaml\"), \"w\"))\n",
        "\n",
        "        training_config = self.config[\"training_config\"]\n",
        "\n",
        "        optimizer = optim.Adam(self.parameters(), lr=training_config[\"optimizer\"][\"lr\"])\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        start_step = 0\n",
        "        stats = defaultdict(list)\n",
        "        if isinstance(ckpt_path, str):\n",
        "            checkpoint = torch.load(ckpt_path, map_location=torch.device('cpu'))\n",
        "            self.load_state_dict(checkpoint[\"model\"])\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "            start_step = checkpoint[\"step\"]\n",
        "            stats = checkpoint[\"stats\"]\n",
        "        if not resume_training:\n",
        "            start_step = 0\n",
        "            stats = defaultdict(list)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=training_config[\"batch_size\"],\n",
        "                                num_workers=training_config[\"num_workers\"], shuffle=True,\n",
        "                                drop_last=training_config[\"drop_last\"], pin_memory=True)\n",
        "\n",
        "        valid_loader = DataLoader(valid_dataset, batch_size=training_config[\"batch_size\"],\n",
        "                                num_workers=training_config[\"num_workers\"], shuffle=True,\n",
        "                                drop_last=training_config[\"drop_last\"], pin_memory=True)\n",
        "\n",
        "        train_loader_iter = iter(train_loader)\n",
        "        for step in trange(start_step, training_config[\"num_steps\"], desc=\"Steps: \"):\n",
        "            try:\n",
        "                batch = next(train_loader_iter)\n",
        "            except StopIteration:\n",
        "                train_loader_iter = iter(train_loader)\n",
        "                batch = next(train_loader_iter)\n",
        "\n",
        "            self.train()\n",
        "\n",
        "            event, mask, index = [t.to(DEVICE) for t in batch]\n",
        "\n",
        "            # defines x and y\n",
        "            x = event\n",
        "            y = mask\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pred = self.forward(x)\n",
        "            loss = criterion(pred, y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            stats[\"train-loss\"].append({\n",
        "               \"step\" : step,\n",
        "               \"loss\" : loss.item()\n",
        "            })\n",
        "\n",
        "            if (step + 1) % training_config[\"valid_interval\"] == 0:\n",
        "                self.eval()\n",
        "\n",
        "                valid_loss = []\n",
        "                for batch in valid_loader:\n",
        "\n",
        "                    event, mask, index = [t.to(DEVICE) for t in batch]\n",
        "\n",
        "                    # defines x and y\n",
        "                    x = event\n",
        "                    y = mask\n",
        "\n",
        "                    pred = self.forward(x)\n",
        "                    loss = criterion(pred, y)\n",
        "\n",
        "                    valid_loss.append(loss.item())\n",
        "\n",
        "                stats[\"valid-loss\"].append({\n",
        "                    \"step\" : step,\n",
        "                    \"loss\" : numpy.mean(valid_loss),\n",
        "                    \"min-loss\" : numpy.min(valid_loss),\n",
        "                    \"max-loss\" : numpy.max(valid_loss),\n",
        "                    \"std-loss\" : numpy.std(valid_loss)\n",
        "                })\n",
        "\n",
        "                # Save if best model so far\n",
        "                if (len(stats[\"valid-loss\"]) == 1) or (stats[\"valid-loss\"][-1][\"loss\"] < numpy.min([l[\"loss\"] for l in stats[\"valid-loss\"][:-1]])):\n",
        "                    checkpoint = {}\n",
        "                    checkpoint['model'] = self.state_dict()\n",
        "                    checkpoint['optimizer'] = optimizer.state_dict()\n",
        "                    checkpoint['step'] = step + 1\n",
        "                    checkpoint['stats'] = stats\n",
        "                    torch.save(checkpoint, os.path.join(model_path, model_name, \"results.pt\"))\n",
        "\n",
        "            if not save_best_ckpt_only:\n",
        "                if (step + 1) % ckpt_period == 0:\n",
        "                    checkpoint = {}\n",
        "                    checkpoint['model'] = self.state_dict()\n",
        "                    checkpoint['optimizer'] = optimizer.state_dict()\n",
        "                    checkpoint['step'] = step + 1\n",
        "                    checkpoint['stats'] = stats\n",
        "                    torch.save(checkpoint, os.path.join(model_path, model_name, \"checkpoint.pt\"))\n",
        "\n",
        "            df = pandas.DataFrame.from_dict(stats[\"train-loss\"])\n",
        "            df.to_csv(os.path.join(quality_control_path, \"train-stats.csv\"))\n",
        "\n",
        "            df = pandas.DataFrame.from_dict(stats[\"valid-loss\"])\n",
        "            df.to_csv(os.path.join(quality_control_path, \"valid-stats.csv\"))\n",
        "\n",
        "    @classmethod\n",
        "    def from_path(cls, model_path, model_name, checkpoint=\"results.pt\"):\n",
        "\n",
        "        checkpoint = torch.load(os.path.join(model_path, model_name, checkpoint), map_location=torch.device('cpu'))\n",
        "        config = yaml.load(open(os.path.join(model_path, model_name, \"config.yaml\"), \"r\"), Loader=yaml.Loader)\n",
        "\n",
        "        model = cls(config)\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "\n",
        "        return model\n",
        "\n",
        "################################################################################################\n",
        "# LOADER DEFINITION\n",
        "################################################################################################\n",
        "\n",
        "def prepare_for_torch(image, full3D=False):\n",
        "    \"\"\"Prepare the image for torch training\n",
        "\n",
        "    Args:\n",
        "        image (numpy.array): the image to prepare\n",
        "\n",
        "    Returns:\n",
        "        image_t (torch.FloatTensor): the image prepared for torch\n",
        "    \"\"\"\n",
        "    if full3D:\n",
        "        image = image[numpy.newaxis]\n",
        "        image_t = torch.from_numpy(numpy.ascontiguousarray(image)).float()\n",
        "        return image_t\n",
        "\n",
        "    if image.ndim == 2:  # pour le cas du mask\n",
        "        image = image[numpy.newaxis]\n",
        "    image_t = torch.from_numpy(numpy.ascontiguousarray(image).astype(numpy.float32))\n",
        "    return image_t\n",
        "\n",
        "class PredictStream3D(Dataset):\n",
        "    \"\"\"Defines the PredictStream dataset\n",
        "\n",
        "    This dataset is useful to help infer full stream.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, stream, shape2crop, step=None):\n",
        "        self.shape2crop = shape2crop\n",
        "        if step is None:\n",
        "            step = numpy.array(shape2crop) // 2\n",
        "        self.step = step\n",
        "\n",
        "        deltaT, deltaH, deltaW = step\n",
        "\n",
        "        T, H, W = stream.shape\n",
        "\n",
        "        limites = ((0, T - shape2crop[0]),\n",
        "                   (0, H - shape2crop[1]),\n",
        "                   (0, W - shape2crop[2]))\n",
        "\n",
        "        T_step = numpy.arange(limites[0][0], limites[0][1], deltaT)\n",
        "        if T_step[-1] is not limites[0][1]:\n",
        "            T_step = numpy.append(T_step, limites[0][1])\n",
        "\n",
        "        H_step = numpy.arange(limites[1][0], limites[1][1], deltaH)\n",
        "        if H_step[-1] is not limites[1][1]:\n",
        "            H_step = numpy.append(H_step, limites[1][1])\n",
        "\n",
        "        W_step = numpy.arange(limites[2][0], limites[2][1], deltaW)\n",
        "        if W_step[-1] is not limites[2][1]:\n",
        "            W_step = numpy.append(W_step, limites[2][1])\n",
        "\n",
        "        # On calcul tous les coins de crop à inférer\n",
        "        corners = list(itertools.product(*[T_step, H_step, W_step]))\n",
        "\n",
        "        self.corners = corners\n",
        "        self.stream = stream\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        corner = self.corners[index]\n",
        "        slicesTHW = (slice(corner[0], corner[0] + self.shape2crop[0]),\n",
        "                     slice(corner[1], corner[1] + self.shape2crop[1]),\n",
        "                     slice(corner[2], corner[2] + self.shape2crop[2]))\n",
        "        crop = self.stream[slicesTHW]\n",
        "        crop = crop[numpy.newaxis]\n",
        "        return prepare_for_torch(crop), torch.from_numpy(numpy.array(corner))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.corners)\n",
        "\n",
        "class MSCTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Creates a `Dataset` to load data from a `h5` dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, h5file, folds, crop_size=64, max_cache_size=64e+9, samples_pu=None, return_full=False, cache_mode=\"normal\"):\n",
        "        \"\"\"\n",
        "        Instantiates `MSCTDataset`\n",
        "\n",
        "        :param h5file: A `h5py.File` of the h5file\n",
        "        :param folds: A `list` of the accessible folds\n",
        "        :param crop_size: An `int` of the size of the crop\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.h5file = h5file\n",
        "        self.folds = folds\n",
        "\n",
        "        if isinstance(crop_size, int):\n",
        "            crop_size = tuple(crop_size for _ in range(3))\n",
        "        self.crop_size = crop_size\n",
        "        self.return_full = return_full\n",
        "\n",
        "        self.cache_mode = cache_mode\n",
        "        avail_cache_mode = [\"normal\", \"full\", \"crop\"]\n",
        "        assert self.cache_mode in avail_cache_mode, f\"This is not a valid cache mode: {avail_cache_mode}\"\n",
        "        self.max_cache_size = max_cache_size\n",
        "        self.cache = {}\n",
        "\n",
        "        self.samples_pu = {}\n",
        "        if isinstance(samples_pu, dict):\n",
        "            self.samples_pu = json.load(open(samples_pu[\"path\"], \"r\"))[f\"{samples_pu['positive']}-1:{samples_pu['unlabeled']}\"]\n",
        "\n",
        "        if self.samples_pu:\n",
        "            if self.cache_mode == \"full\":\n",
        "                self.info = self.get_file_info_pu_cache_mode_full()\n",
        "            else:\n",
        "                self.info = self.get_file_info_pu()\n",
        "        else:\n",
        "            # This is assumed that HDF5 file only contains crops in\n",
        "            # data/label keys\n",
        "            if self.cache_mode == \"crop\":\n",
        "                self.info = self.get_file_info_cache_mode_crop()\n",
        "            else:\n",
        "                self.info = self.get_file_info()\n",
        "\n",
        "    def _getsizeof(self, obj):\n",
        "        \"\"\"\n",
        "        Returns the size of an array\n",
        "        :param ary: A `numpy.ndarray`\n",
        "        :returns : The size of the array in `bytes`\n",
        "        \"\"\"\n",
        "        if isinstance(obj, (list, tuple)):\n",
        "            return sum([self._getsizeof(o) for o in obj])\n",
        "        if isinstance(obj, str):\n",
        "            return len(str)\n",
        "        return obj.size * obj.dtype.itemsize\n",
        "\n",
        "    def _calc_current_cache_size(self, info):\n",
        "        \"\"\"\n",
        "        Calculates the current cache size\n",
        "        \"\"\"\n",
        "        if self.max_cache_size > 0:\n",
        "            return sum([di[\"datasize\"]\n",
        "                        for data_infos in info.values()\n",
        "                        for di in data_infos\n",
        "                        if isinstance(di[\"cache-idx\"], str)])\n",
        "        return 0\n",
        "\n",
        "    def _calc_cached_items(self, info):\n",
        "        if self.max_cache_size > 0:\n",
        "            return sum([1\n",
        "                        for data_infos in info.values()\n",
        "                        for di in data_infos\n",
        "                        if isinstance(di['cache-idx'], str)])\n",
        "        return 0\n",
        "\n",
        "    def get_file_info(self):\n",
        "        \"\"\"\n",
        "        Extracts the file information\n",
        "        \"\"\"\n",
        "\n",
        "        info = {\n",
        "            \"input\" : [],\n",
        "            \"label\" : []\n",
        "        }\n",
        "        print(\"Getting file information... This may take a while...\")\n",
        "        with h5py.File(self.h5file, \"r\") as file:\n",
        "            for key in info.keys():\n",
        "                print(f\"Getting dataset: {key}\")\n",
        "                for fold in self.folds:\n",
        "                    print(f\"Getting fold: {fold}\")\n",
        "                    if isinstance(fold, str):\n",
        "                        for gg, group in enumerate(tqdm(sorted(file[fold].keys(), key=lambda key : int(key)))):\n",
        "                            if self.return_full:\n",
        "                                event = file[fold][group][key]\n",
        "                                info[key].append({\n",
        "                                    \"fold\" : fold,\n",
        "                                    \"group\" : group,\n",
        "                                    \"shape\" : file[fold][group][key].shape,\n",
        "                                    \"event-idx\" : None,\n",
        "                                    \"key\" : \"/\".join((fold, group, key)),\n",
        "                                    \"datasize\" : self._getsizeof(event),\n",
        "                                    \"cache-idx\" : None,\n",
        "                                    \"is-empty\" : False\n",
        "                                })\n",
        "                            else:\n",
        "                                if f\"cache-{key}\" in file[fold][group]:\n",
        "                                    for ee, (idx, event) in enumerate(tqdm(file[fold][group][f\"cache-{key}\"].items(), leave=False)):\n",
        "                                        cache_idx = None\n",
        "                                        is_empty = not numpy.any(event)\n",
        "                                        current_cache_size = self._calc_current_cache_size(info)\n",
        "                                        if (not is_empty) and (current_cache_size < self.max_cache_size):\n",
        "                                            cache_idx = \"/\".join((fold, group, f\"cache-{key}\", idx))\n",
        "                                            self.cache[cache_idx] = event[()]\n",
        "                                        info[key].append({\n",
        "                                            \"fold\" : fold,\n",
        "                                            \"group\" : group,\n",
        "                                            \"shape\" : file[fold][group][f\"cache-{key}\"][idx].shape,\n",
        "                                            \"event-idx\" : idx,\n",
        "                                            \"key\" : \"/\".join((fold, group, f\"cache-{key}\", idx)),\n",
        "                                            \"datasize\" : self._getsizeof(event),\n",
        "                                            \"cache-idx\" : cache_idx,\n",
        "                                            \"is-empty\" : is_empty\n",
        "                                        })\n",
        "                                else:\n",
        "                                    for ee, (idx, event) in enumerate(tqdm(file[fold][group][key].items(), leave=False)):\n",
        "                                        cache_idx = None\n",
        "                                        is_empty = not numpy.any(event)\n",
        "                                        current_cache_size = self._calc_current_cache_size(info)\n",
        "                                        if (not is_empty) and (current_cache_size < self.max_cache_size):\n",
        "                                            cache_idx = \"/\".join((fold, group, key, idx))\n",
        "                                            self.cache[cache_idx] = event[()]\n",
        "                                        info[key].append({\n",
        "                                            \"fold\" : fold,\n",
        "                                            \"group\" : group,\n",
        "                                            \"shape\" : file[fold][group][key][idx].shape,\n",
        "                                            \"event-idx\" : idx,\n",
        "                                            \"key\" : \"/\".join((fold, group, key, idx)),\n",
        "                                            \"datasize\" : self._getsizeof(event),\n",
        "                                            \"cache-idx\" : cache_idx,\n",
        "                                            \"is-empty\" : is_empty\n",
        "                                        })\n",
        "        #                             print(f\"[----] Neuron: {group} ({100 * (gg + 1) / len(file[fold]):0.1f});\")\n",
        "                                print(f\"Current cache size: {self._calc_current_cache_size(info) * 1e-9:0.2f}G; Cached items: {self._calc_cached_items(info)});\")\n",
        "        return info\n",
        "\n",
        "    def get_file_info_cache_mode_crop(self):\n",
        "        \"\"\"\n",
        "        Extracts the file information\n",
        "        \"\"\"\n",
        "\n",
        "        info = {\n",
        "            \"input\" : [],\n",
        "            \"label\" : []\n",
        "        }\n",
        "        print(\"Getting file information... This may take a while...\")\n",
        "        with h5py.File(self.h5file, \"r\") as file:\n",
        "            for key in info.keys():\n",
        "                print(f\"Getting dataset: {key}\")\n",
        "                for fold in self.folds:\n",
        "                    print(f\"Getting fold: {fold}\")\n",
        "                    if isinstance(fold, str):\n",
        "                        for gg, group in enumerate(tqdm(sorted(file[fold].keys(), key=lambda key : int(key)))):\n",
        "\n",
        "                            for event in file[fold][group][\"events\"]:\n",
        "                                event_idx, event = str(event[:1]), event[1:]\n",
        "\n",
        "                                event_center = event.reshape(-1, 2)\n",
        "                                event_center = numpy.mean(event_center, axis=-1).astype(int)\n",
        "                                slc = tuple(\n",
        "                                    slice(max(0, c - s // 2), min(_max, c + s // 2)) for c, s, _max in zip(event_center, self.crop_size, file[fold][group][key].shape)\n",
        "                                )\n",
        "\n",
        "                                event = file[fold][group][key][slc]\n",
        "\n",
        "                                cache_idx = None\n",
        "                                is_empty = not numpy.any(event)\n",
        "                                current_cache_size = self._calc_current_cache_size(info)\n",
        "                                if (not is_empty) and (current_cache_size < self.max_cache_size):\n",
        "                                    cache_idx = \"/\".join((fold, group, key, event_idx))\n",
        "                                    self.cache[cache_idx] = event\n",
        "\n",
        "                                info[key].append({\n",
        "                                    \"fold\" : fold,\n",
        "                                    \"group\" : group,\n",
        "                                    \"shape\" : self.crop_size,\n",
        "                                    \"event-idx\" : event_idx,\n",
        "                                    \"key\" : \"/\".join((fold, group, key)),\n",
        "                                    \"datasize\" : self._getsizeof(event),\n",
        "                                    \"cache-idx\" : cache_idx,\n",
        "                                    \"is-empty\" : is_empty,\n",
        "                                    \"slice\" : slc\n",
        "                                })\n",
        "\n",
        "                            print(f\"Current cache size: {self._calc_current_cache_size(info) * 1e-9:0.2f}G; Cached items: {self._calc_cached_items(info)});\")\n",
        "        return info\n",
        "\n",
        "    def get_file_info_pu(self):\n",
        "        info = {\n",
        "            \"input\" : [],\n",
        "            \"label\" : []\n",
        "        }\n",
        "        print(\"Getting file information... This may take a while...\")\n",
        "        with h5py.File(self.h5file, \"r\") as file:\n",
        "            for key in info.keys():\n",
        "                print(f\"Getting dataset: {key}\")\n",
        "                for fold in self.folds:\n",
        "                    print(f\"Getting fold: {fold}\")\n",
        "                    if isinstance(fold, str):\n",
        "                        # Adds positive samples\n",
        "                        for sample in self.samples_pu[\"positive\"][fold]:\n",
        "                            cache_idx = None\n",
        "                            group = sample[\"neuron\"]\n",
        "                            idx = sample[\"event-id\"]\n",
        "                            event = file[fold][group][\"events\"][idx]\n",
        "\n",
        "                            cache_idx = \"/\".join((fold, group, f\"cache-{key}\", str(idx)))\n",
        "                            event = file[cache_idx]\n",
        "                            is_empty = not numpy.any(event)\n",
        "\n",
        "                            current_cache_size = self._calc_current_cache_size(info)\n",
        "                            if (not is_empty) and (current_cache_size < self.max_cache_size):\n",
        "                                cache_idx = \"/\".join((fold, group, f\"cache-{key}\", str(idx)))\n",
        "                                self.cache[cache_idx] = file[cache_idx][()]\n",
        "                            else:\n",
        "                                cache_idx = None\n",
        "\n",
        "                            idx = str(idx)\n",
        "                            info[key].append({\n",
        "                                \"fold\" : fold,\n",
        "                                \"group\" : group,\n",
        "                                \"shape\" : file[fold][group][f\"cache-{key}\"][idx].shape,\n",
        "                                \"event-idx\" : idx,\n",
        "                                \"key\" : \"/\".join((fold, group, f\"cache-{key}\", idx)),\n",
        "                                \"datasize\" : self._getsizeof(event),\n",
        "                                \"cache-idx\" : cache_idx,\n",
        "                                \"is-empty\" : is_empty\n",
        "                            })\n",
        "\n",
        "                        print(f\"[----] Current cache size: {self._calc_current_cache_size(info) * 1e-9:0.2f}G; Cached items: {self._calc_cached_items(info)});\")\n",
        "\n",
        "                        # Adds unlabeled samples\n",
        "                        for idx, sample in enumerate(self.samples_pu[\"negative\"][fold]):\n",
        "                            group = sample[\"neuron\"]\n",
        "                            coord = sample[\"coord\"]\n",
        "                            idx = str(coord)\n",
        "\n",
        "                            cache_idx = \"/\".join((fold, group, f\"cache-unlabeled-{key}\", idx))\n",
        "                            current_cache_size = self._calc_current_cache_size(info)\n",
        "                            if not (cache_idx in file):\n",
        "                                cache_idx = None\n",
        "                                is_empty = False\n",
        "                                slc = tuple(slice(coord[i], coord[i] + self.crop_size[i]) for i in range(3))\n",
        "                                datasize = 0\n",
        "                                if current_cache_size < self.max_cache_size:\n",
        "                                    cache_idx = \"/\".join((fold, group, f\"cache-unlabeled-{key}\", idx))\n",
        "                                    event = file[\"/\".join((fold, group, key))][slc]\n",
        "                                    is_empty = not numpy.any(event)\n",
        "                                    if not is_empty:\n",
        "                                        self.cache[cache_idx] = event\n",
        "                                    else:\n",
        "                                        cache_idx = None\n",
        "\n",
        "                                    datasize = self._getsizeof(event)\n",
        "\n",
        "                                info[key].append({\n",
        "                                    \"fold\" : fold,\n",
        "                                    \"group\" : group,\n",
        "                                    \"shape\" : self.crop_size,\n",
        "                                    \"event-idx\" : idx,\n",
        "                                    \"key\" : \"/\".join((fold, group, key)),\n",
        "                                    \"datasize\" : datasize,\n",
        "                                    \"cache-idx\" : cache_idx,\n",
        "                                    \"is-empty\" : is_empty,\n",
        "                                    \"slice\" : slc\n",
        "                                })\n",
        "                            else:\n",
        "                                event = file[cache_idx]\n",
        "                                is_empty = not numpy.any(event)\n",
        "                                if (not is_empty) and (current_cache_size < self.max_cache_size):\n",
        "                                    self.cache[cache_idx] = event[()]\n",
        "                                else:\n",
        "                                    cache_idx = None\n",
        "\n",
        "                                info[key].append({\n",
        "                                    \"fold\" : fold,\n",
        "                                    \"group\" : group,\n",
        "                                    \"shape\" : file[fold][group][f\"cache-unlabeled-{key}\"][idx].shape,\n",
        "                                    \"event-idx\" : idx,\n",
        "                                    \"key\" : \"/\".join((fold, group, f\"cache-unlabeled-{key}\", idx)),\n",
        "                                    \"datasize\" : self._getsizeof(event),\n",
        "                                    \"cache-idx\" : cache_idx,\n",
        "                                    \"is-empty\" : is_empty\n",
        "                                })\n",
        "\n",
        "                        print(f\"[----] Current cache size: {self._calc_current_cache_size(info) * 1e-9:0.2f}G; Cached items: {self._calc_cached_items(info)});\")\n",
        "        return info\n",
        "\n",
        "    def get_file_info_pu_cache_mode_full(self):\n",
        "        info = {\n",
        "            \"input\" : [],\n",
        "            \"label\" : []\n",
        "        }\n",
        "        print(\"Getting file information... This may take a while...\")\n",
        "        with h5py.File(self.h5file, \"r\") as file:\n",
        "            for key in info.keys():\n",
        "                print(f\"Getting dataset: {key}\")\n",
        "                for fold in self.folds:\n",
        "                    print(f\"Getting fold: {fold}\")\n",
        "                    if isinstance(fold, str):\n",
        "                        # Adds positive samples\n",
        "                        for sample in self.samples_pu[\"positive\"][fold]:\n",
        "                            cache_idx = None\n",
        "                            group = sample[\"neuron\"]\n",
        "                            idx = sample[\"event-id\"]\n",
        "                            event = file[fold][group][\"events\"][idx]\n",
        "\n",
        "                            cache_idx = \"/\".join((fold, group, f\"cache-{key}\", str(idx)))\n",
        "                            event = file[cache_idx]\n",
        "                            is_empty = not numpy.any(event)\n",
        "\n",
        "                            current_cache_size = self._calc_current_cache_size(info)\n",
        "                            if (not is_empty) and (current_cache_size < self.max_cache_size):\n",
        "                                cache_idx = \"/\".join((fold, group, f\"cache-{key}\", str(idx)))\n",
        "                                self.cache[cache_idx] = file[cache_idx][()]\n",
        "                            else:\n",
        "                                cache_idx = None\n",
        "\n",
        "                            idx = str(idx)\n",
        "                            info[key].append({\n",
        "                                \"fold\" : fold,\n",
        "                                \"group\" : group,\n",
        "                                \"shape\" : file[fold][group][f\"cache-{key}\"][idx].shape,\n",
        "                                \"event-idx\" : idx,\n",
        "                                \"key\" : \"/\".join((fold, group, f\"cache-{key}\", idx)),\n",
        "                                \"datasize\" : self._getsizeof(event),\n",
        "                                \"cache-idx\" : cache_idx,\n",
        "                                \"is-empty\" : is_empty\n",
        "                            })\n",
        "\n",
        "                        print(f\"[----] Current cache size: {self._calc_current_cache_size(info) * 1e-9:0.2f}G; Cached items: {self._calc_cached_items(info)});\")\n",
        "\n",
        "                        # Adds unlabeled samples\n",
        "                        for idx, sample in enumerate(tqdm(self.samples_pu[\"negative\"][fold])):\n",
        "                            group = sample[\"neuron\"]\n",
        "                            coord = sample[\"coord\"]\n",
        "                            idx = str(coord)\n",
        "\n",
        "                            slc = tuple(slice(coord[i], coord[i] + self.crop_size[i]) for i in range(3))\n",
        "\n",
        "                            # current_cache_size = self._calc_current_cache_size(info)\n",
        "                            cache_idx = \"/\".join((fold, group, key))\n",
        "                            if not (cache_idx in self.cache) and True:#(current_cache_size < self.max_cache_size):\n",
        "                                self.cache[cache_idx] = file[fold][group][key][()]\n",
        "                                datasize = self._getsizeof(self.cache[cache_idx])\n",
        "                            elif (cache_idx in self.cache):\n",
        "                                datasize = 0\n",
        "                            else:\n",
        "                                datasize = 0\n",
        "                                cache_idx = None\n",
        "\n",
        "                            info[key].append({\n",
        "                                \"fold\" : fold,\n",
        "                                \"group\" : group,\n",
        "                                \"shape\" : self.crop_size,\n",
        "                                \"event-idx\" : idx,\n",
        "                                \"key\" : \"/\".join((fold, group, key)),\n",
        "                                \"datasize\" : datasize,\n",
        "                                \"cache-idx\" : cache_idx,\n",
        "                                \"is-empty\" : False,\n",
        "                                \"slice\" : slc\n",
        "                            })\n",
        "\n",
        "\n",
        "                        print(f\"[----] Current cache size: {self._calc_current_cache_size(info) * 1e-9:0.2f}G; Cached items: {self._calc_cached_items(info)});\")\n",
        "        return info\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Implements the getitem method of the `MSCTSequence`\n",
        "\n",
        "        :param index: An `int` of the index\n",
        "        \"\"\"\n",
        "\n",
        "        info_input = self.info[\"input\"][index]\n",
        "        info_label = self.info[\"label\"][index]\n",
        "\n",
        "        # Crop input\n",
        "        if isinstance(info_input[\"cache-idx\"], str):\n",
        "            crop_input = self.cache[info_input[\"cache-idx\"]]\n",
        "            if (self.cache_mode == \"full\") and (\"slice\" in info_input):\n",
        "                crop_input = crop_input[info_input[\"slice\"]]\n",
        "        else:\n",
        "            with h5py.File(self.h5file, \"r\") as file:\n",
        "                if (self.cache_mode == \"crop\") and (\"slice\" in info_input):\n",
        "                    crop_input = file[info_input[\"key\"]][info_input[\"slice\"]]\n",
        "                else:\n",
        "                    crop_input = file[info_input[\"key\"]][()]\n",
        "\n",
        "        # Crop label\n",
        "        if info_label[\"is-empty\"]:\n",
        "            crop_label = numpy.zeros(self.crop_size, dtype=numpy.uint8)\n",
        "        elif isinstance(info_label[\"cache-idx\"], str):\n",
        "            crop_label = self.cache[info_label[\"cache-idx\"]]\n",
        "            if (self.cache_mode == \"full\") and (\"slice\" in info_label):\n",
        "                crop_label = crop_label[info_label[\"slice\"]]\n",
        "        else:\n",
        "            with h5py.File(self.h5file, \"r\") as file:\n",
        "                if (self.cache_mode == \"crop\") and (\"slice\" in info_label):\n",
        "                    crop_label = file[info_label[\"key\"]][info_label[\"slice\"]]\n",
        "                else:\n",
        "                    crop_label = file[info_label[\"key\"]][()]\n",
        "        crop_label = crop_label > 0\n",
        "\n",
        "        if not self.return_full:\n",
        "            # Pads crop if not good size\n",
        "            if crop_input.size != numpy.prod(self.crop_size):\n",
        "                crop_input = numpy.pad(\n",
        "                    crop_input,\n",
        "                    [(0, cs - current) for cs, current in zip(self.crop_size, crop_input.shape)],\n",
        "                    mode=\"symmetric\"\n",
        "                )\n",
        "            if crop_label.size != numpy.prod(self.crop_size):\n",
        "                crop_label = numpy.pad(\n",
        "                    crop_label,\n",
        "                    [(0, cs - current) for cs, current in zip(self.crop_size, crop_label.shape)],\n",
        "                    mode=\"symmetric\"\n",
        "                )\n",
        "        volume = crop_input.astype(numpy.float32)\n",
        "        label = crop_label.astype(numpy.uint8)\n",
        "\n",
        "        return {\n",
        "            \"input\" : volume,\n",
        "            \"label\" : label,\n",
        "            \"detection\" : None,\n",
        "            \"index\" : index,\n",
        "            \"shape2crop\" : numpy.array([s // 2 for s in self.crop_size])\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.info[\"input\"])\n",
        "\n",
        "class LoadedEvent3D(Dataset):\n",
        "    \"\"\"\n",
        "    Defines the LoadedEvent dataset. Si on veut ajouter un channel de detection\n",
        "    en entrainement, on va tout simplement retourner un mask de deux channels\n",
        "\n",
        "    :param data: `list` List of loaded events\n",
        "    :param transform: (torchvision.transforms): Transforms to apply on the dataset\n",
        "    :param center_crop: (bool): Whether a center crop is forced\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, transforms=None):\n",
        "        self.data = data\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        event_dict = self.data[index]\n",
        "\n",
        "        event = event_dict['input']\n",
        "        mask = event_dict['label']\n",
        "        eventIndex = event_dict['index']\n",
        "\n",
        "        if self.transforms:\n",
        "            event, mask = self.transforms((event, mask))\n",
        "\n",
        "        event = prepare_for_torch(event, full3D=True)\n",
        "        mask = prepare_for_torch(mask, full3D=True)\n",
        "\n",
        "        return event, mask, int(eventIndex)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "class MSCTDatasetFromFolder(Dataset):\n",
        "    \"\"\"\n",
        "    Creates a `Dataset` to load data from a folder of tifffiles\n",
        "    \"\"\"\n",
        "    def __init__(self, source_folder, target_folder, crop_size=64):\n",
        "        \"\"\"\n",
        "        Instantiates `MSCTDatasetFromFolder`\n",
        "\n",
        "        :param folds: A `list` of the accessible folds\n",
        "        :param crop_size: An `int` of the size of the crop\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.source_folder = source_folder\n",
        "        self.target_folder = target_folder\n",
        "\n",
        "        self.source_files = glob.glob(os.path.join(self.source_folder, \"*.tif\"))\n",
        "        self.target_files = [file.replace(self.source_folder, self.target_folder) for file in self.source_folder]\n",
        "\n",
        "        remove_idx = []\n",
        "        for i, (source_file, target_file) in enumerate(zip(self.source_files, self.target_files)):\n",
        "            if not os.path.isfile(target_file):\n",
        "                remove_idx.append(i)\n",
        "\n",
        "        for idx in reversed(remove_idx):\n",
        "            print(f\"Source file: {self.source_files[idx]} does not have an annotated file... Removing\")\n",
        "            del self.source_files[idx]\n",
        "            del self.target_files[idx]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Implements the getitem method of the `MSCTSequence`\n",
        "\n",
        "        :param index: An `int` of the index\n",
        "        \"\"\"\n",
        "\n",
        "        crop_input = tifffile.imread(self.source_files[index])\n",
        "        crop_label = tifffile.imread(self.target_files[index]) > 0\n",
        "\n",
        "        # Pads crop if not good size\n",
        "        if crop_input.size != numpy.prod(self.crop_size):\n",
        "            crop_input = numpy.pad(\n",
        "                crop_input,\n",
        "                [(0, cs - current) for cs, current in zip(self.crop_size, crop_input.shape)],\n",
        "                mode=\"symmetric\"\n",
        "            )\n",
        "        if crop_label.size != numpy.prod(self.crop_size):\n",
        "            crop_label = numpy.pad(\n",
        "                crop_label,\n",
        "                [(0, cs - current) for cs, current in zip(self.crop_size, crop_label.shape)],\n",
        "                mode=\"symmetric\"\n",
        "            )\n",
        "        volume = crop_input.astype(numpy.float32)\n",
        "        label = crop_label.astype(numpy.uint8)\n",
        "\n",
        "        return {\n",
        "            \"input\" : volume,\n",
        "            \"label\" : label,\n",
        "            \"detection\" : None,\n",
        "            \"index\" : index,\n",
        "            \"shape2crop\" : numpy.array([s // 2 for s in self.crop_size])\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_files)\n",
        "\n",
        "class RandomFlip(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.possible_flips = (-2, -1, None) # 1 flipud, 2 fliplr, None nothing\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, y = x\n",
        "\n",
        "        flipping_mode = random.choice(self.possible_flips)\n",
        "        if flipping_mode:\n",
        "            x = numpy.flip(x, flipping_mode)\n",
        "            y = numpy.flip(y, flipping_mode)\n",
        "\n",
        "        return (x, y)\n",
        "\n",
        "class RandomCrop(nn.Module):\n",
        "    def __init__(self, crop_size) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.crop_size = crop_size\n",
        "        if isinstance(self.crop_size, int):\n",
        "            self.crop_size = numpy.array([self.crop_size for _ in range(3)])\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x, y = x\n",
        "\n",
        "        eventShape = numpy.array(x.shape)\n",
        "        choices = eventShape - self.crop_size + 1\n",
        "        corner = numpy.array([numpy.random.randint(low=0, high=d) for d in choices])\n",
        "\n",
        "        slicesTHW = tuple([slice(corner[i], corner[i] + self.crop_size[i]) for i in range(3)])\n",
        "\n",
        "        x = x[slicesTHW]\n",
        "        y = y[slicesTHW]\n",
        "        return (x, y)\n",
        "\n",
        "class CenterCrop(nn.Module):\n",
        "    def __init__(self, crop_size) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.crop_size = crop_size\n",
        "        if isinstance(self.crop_size, int):\n",
        "            self.crop_size = numpy.array([self.crop_size for _ in range(3)])\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x, y = x\n",
        "\n",
        "        eventShape = numpy.array(x.shape)\n",
        "        center = eventShape // 2\n",
        "        corner = center - self.crop_size // 2\n",
        "\n",
        "        slicesTHW = tuple([slice(corner[i], corner[i] + self.crop_size[i]) for i in range(3)])\n",
        "\n",
        "        x = x[slicesTHW]\n",
        "        y = y[slicesTHW]\n",
        "        return (x, y)\n",
        "\n",
        "def baseline(y, lam=1e3, ratio=1e-6):\n",
        "    \"\"\"\n",
        "    Provient de https://github.com/charlesll/rampy/blob/master/rampy/baseline.py\n",
        "    \"\"\"\n",
        "    N = len(y)\n",
        "    D = scipy.sparse.csc_matrix(numpy.diff(numpy.eye(N), 2))\n",
        "    w = numpy.ones(N)\n",
        "    MAX_ITER = 100\n",
        "\n",
        "    for _ in range(MAX_ITER):\n",
        "        W = scipy.sparse.spdiags(w, 0, N, N)\n",
        "        Z = W + lam * D.dot(D.transpose())\n",
        "        z = scipy.sparse.linalg.spsolve(Z, w * y)\n",
        "        d = y - z\n",
        "        # make d- and get w^t with m and s\n",
        "        dn = d[d < 0]\n",
        "        m = numpy.mean(dn)\n",
        "        s = numpy.std(dn)\n",
        "        wt = 1.0 / (1 + numpy.exp(2 * (d - (2 * s - m)) / s))\n",
        "        # check exit condition and backup\n",
        "        if numpy.linalg.norm(w - wt) / numpy.linalg.norm(w) < ratio:\n",
        "            break\n",
        "        w = wt\n",
        "\n",
        "    return z\n",
        "\n",
        "def preprocess_stream(stream, method='moving_statistic', kernel_size=None):\n",
        "    \"\"\"\n",
        "    Encapsulate all preprocessing steps\n",
        "    \"\"\"\n",
        "    stream = stream.astype(numpy.float32)\n",
        "    if method == 'moving_statistic':\n",
        "        # Find threshold\n",
        "        threshold = filters.threshold_triangle(stream)\n",
        "        foreground_intensity = numpy.mean(stream[stream > threshold])\n",
        "        mean_by_frame = numpy.mean(stream, axis=(1, 2))\n",
        "        mean_average = baseline(mean_by_frame)\n",
        "        mean_average = mean_average[..., numpy.newaxis, numpy.newaxis]\n",
        "        stream = (stream - mean_average) / foreground_intensity\n",
        "    return stream\n",
        "\n",
        "def filter_regionprops(regionprops, constraints):\n",
        "    updated_regionprops, remove_coords = [], []\n",
        "    for rprop in regionprops:\n",
        "        t1, h1, w1, t2, h2, w2 = rprop.bbox\n",
        "        lenT = t2 - t1\n",
        "        lenH = h2 - h1\n",
        "        lenW = w2 - w1\n",
        "        should_remove = False\n",
        "        if \"minimal_time\" in constraints:\n",
        "            if lenT < constraints[\"minimal_time\"]:\n",
        "                should_remove = True\n",
        "            if lenH < constraints[\"minimal_height\"]:\n",
        "                should_remove = True\n",
        "            if lenW < constraints[\"minimal_width\"]:\n",
        "                should_remove = True\n",
        "        if should_remove:\n",
        "            remove_coords.extend(rprop.coords)\n",
        "        else:\n",
        "            updated_regionprops.append(rprop)\n",
        "    return updated_regionprops\n",
        "\n",
        "def compute_optimal_thresholds_detection(truth, pred):\n",
        "    \"\"\"\n",
        "    Computes the optimal threshold to apply the prediction to match the ground\n",
        "    truth mask\n",
        "\n",
        "    :param truth: A `numpy.ndarray` of the ground truth mask\n",
        "    :param pred: A `numpy.ndarray` of the predicted segmentation\n",
        "\n",
        "    :returns : A `float` of the threshold\n",
        "    \"\"\"\n",
        "    if isinstance(truth, torch.Tensor):\n",
        "        truth = truth.cpu().data.numpy()\n",
        "    if isinstance(pred, torch.Tensor):\n",
        "        pred = pred.cpu().data.numpy()\n",
        "\n",
        "    thresholds = numpy.linspace(0.01, 1, 100)\n",
        "    optimal_threshold, best_f1 = thresholds[0], -1\n",
        "    for t in tqdm(thresholds, desc=\"Thresholds\", leave=False):\n",
        "        f1 = _compute_optimal_thresholds_detection(truth, pred, t)\n",
        "        if f1 >= best_f1:\n",
        "            optimal_threshold = t\n",
        "            best_f1 = f1\n",
        "    return optimal_threshold\n",
        "\n",
        "def _compute_optimal_thresholds_detection(truth, pred, threshold):\n",
        "    \"\"\"\n",
        "    Computes the optimal threshold to apply the prediction to match the ground\n",
        "    truth mask\n",
        "\n",
        "    :param truth: A `numpy.ndarray` of the ground truth mask\n",
        "    :param pred: A `numpy.ndarray` of the predicted segmentation\n",
        "\n",
        "    :returns : A `float` of the threshold\n",
        "    \"\"\"\n",
        "    truth, pred = truth.squeeze(), pred.squeeze()\n",
        "\n",
        "    pred = pred > threshold\n",
        "\n",
        "    # There are no predictions\n",
        "    if not numpy.any(pred):\n",
        "        return 0\n",
        "\n",
        "    truth_label = measure.label(truth)\n",
        "    truth_rprops = measure.regionprops(truth_label)\n",
        "    truth_centroids = [rprop.centroid for rprop in truth_rprops]\n",
        "\n",
        "    pred_label = measure.label(pred)\n",
        "    pred_rprops = measure.regionprops(pred_label)\n",
        "\n",
        "    # Filter small regions\n",
        "    POSTPROCESS_PARAMS = {\n",
        "        \"minimal_time\" : 2,\n",
        "        \"minimal_height\" : 3,\n",
        "        \"minimal_width\" : 3\n",
        "    }\n",
        "    pred_rprops = filter_regionprops(pred_rprops, POSTPROCESS_PARAMS)\n",
        "\n",
        "    pred_centroids = []\n",
        "    for rprop in pred_rprops:\n",
        "        try:\n",
        "            centroid = rprop.centroid\n",
        "        except:\n",
        "            centroid = rprop.bbox.reshape(-1, 2)\n",
        "            centroid = numpy.mean(centroid, axis=-1)\n",
        "        pred_centroids.append(centroid)\n",
        "    # pred_centroids = [rprop.centroid for rprop in pred_rprops]\n",
        "\n",
        "    scorer = CentroidDetectionError(truth_centroids, pred_centroids, threshold=6)\n",
        "\n",
        "    return scorer.f1_score\n",
        "\n",
        "def compute_optimal_thresholds(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes the optimal thresholds for every item using a precision recall curve\n",
        "\n",
        "    :param y_true: A `torch.Tensor` of ground truth\n",
        "    :param y_pred: A `torch.Tensor` of prediction\n",
        "\n",
        "    :returns : A `list` of thresholds\n",
        "    \"\"\"\n",
        "    if isinstance(y_true, torch.Tensor):\n",
        "        y_true = y_true.cpu().data.numpy()\n",
        "    if isinstance(y_pred, torch.Tensor):\n",
        "        y_pred = y_pred.cpu().data.numpy()\n",
        "\n",
        "    thresholds = []\n",
        "    for truth, pred in zip(y_true, y_pred):\n",
        "        if numpy.any(truth):\n",
        "            thresholds.append(_compute_optimal_threshold(truth, pred))\n",
        "    return thresholds\n",
        "\n",
        "def _compute_optimal_threshold(truth, pred):\n",
        "    \"\"\"\n",
        "    Computes the optimal threshold to apply the prediction to match the ground\n",
        "    truth mask\n",
        "\n",
        "    :param truth: A `numpy.ndarray` of the ground truth mask\n",
        "    :param pred: A `numpy.ndarray` of the predicted segmentation\n",
        "\n",
        "    :returns : A `float` of the threshold\n",
        "    \"\"\"\n",
        "    precision, recall, thresholds = precision_recall_curve(truth.ravel(), pred.ravel())\n",
        "    distances = distance.cdist(numpy.stack((precision, recall), axis=-1), numpy.array([[1., 1.]])).ravel()\n",
        "    return thresholds[distances.argmin()]\n",
        "\n",
        "# Below are templates for the function definitions for the export\n",
        "# of pdf summaries for training and qc. You will need to adjust these functions\n",
        "# with the variables and other parameters as necessary to make them\n",
        "# work for your project\n",
        "from datetime import datetime\n",
        "\n",
        "def pdf_export(trained = False, augmentation = False, pretrained_model = False):\n",
        "    # save FPDF() class into a\n",
        "    # variable pdf\n",
        "    #from datetime import datetime\n",
        "\n",
        "    class MyFPDF(FPDF, HTMLMixin):\n",
        "        pass\n",
        "\n",
        "    pdf = MyFPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_right_margin(-1)\n",
        "    pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "\n",
        "    Network = \"Calcium U-Net3D\"\n",
        "    day = datetime.now()\n",
        "    datetime_str = str(day)[0:10]\n",
        "\n",
        "    Header = 'Training report for '+Network+' model ('+model_name+')\\nDate: '+datetime_str\n",
        "    pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
        "    pdf.ln(1)\n",
        "\n",
        "    # add another cell\n",
        "    if trained:\n",
        "        training_time = \"Training time: \"+str(hour)+ \"hour(s) \"+str(mins)+\"min(s) \"+str(round(sec))+\"sec(s)\"\n",
        "        pdf.cell(190, 5, txt = training_time, ln = 1, align='L')\n",
        "        pdf.ln(1)\n",
        "\n",
        "    Header_2 = 'Information for your materials and methods:'\n",
        "    pdf.cell(190, 5, txt=Header_2, ln=1, align='L')\n",
        "    pdf.ln(1)\n",
        "\n",
        "    all_packages = ''\n",
        "    for requirement in freeze(local_only=True):\n",
        "        all_packages = all_packages+requirement+', '\n",
        "    #print(all_packages)\n",
        "\n",
        "    #Main Packages\n",
        "    main_packages = ''\n",
        "    version_numbers = []\n",
        "    for name in ['tensorflow','numpy','Keras','csbdeep']:\n",
        "        find_name=all_packages.find(name)\n",
        "        main_packages = main_packages+all_packages[find_name:all_packages.find(',',find_name)]+', '\n",
        "        #Version numbers only here:\n",
        "        version_numbers.append(all_packages[find_name+len(name)+2:all_packages.find(',',find_name)])\n",
        "    try:\n",
        "        cuda_version = subprocess.run('nvcc --version',stdout=subprocess.PIPE, shell=True)\n",
        "        cuda_version = cuda_version.stdout.decode('utf-8')\n",
        "        cuda_version = cuda_version[cuda_version.find(', V')+3:-1]\n",
        "    except:\n",
        "        cuda_version = \" - No cuda found - \"\n",
        "    try:\n",
        "        gpu_name = subprocess.run('nvidia-smi',stdout=subprocess.PIPE, shell=True)\n",
        "        gpu_name = gpu_name.stdout.decode('utf-8')\n",
        "        gpu_name = gpu_name[gpu_name.find('Tesla'):gpu_name.find('Tesla')+10]\n",
        "    except:\n",
        "        gpu_name = \" - No GPU found - \"\n",
        "    #print(cuda_version[cuda_version.find(', V')+3:-1])\n",
        "    #print(gpu_name)\n",
        "\n",
        "    if os.path.isdir(Training_source):\n",
        "        shape = tifffile.imread(Training_source+'/'+os.listdir(Training_source)[0]).shape\n",
        "    elif os.path.isfile(Training_source):\n",
        "        shape = (patch_size, patch_size, patch_size)\n",
        "    else:\n",
        "        print('Cannot read training data.')\n",
        "\n",
        "    dataset_size = len(train_generator) + len(valid_generator)\n",
        "\n",
        "    text = 'The '+Network+' model was trained from scratch for '+str(number_of_steps)+' steps on '+str(dataset_size)+' paired image patches (image dimensions: '+str(shape)+', patch size: ('+str(patch_size)+')) with a batch size of '+str(batch_size)+' and a MSELoss loss function, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). Key python packages used include tensorflow (v '+version_numbers[0]+'), Keras (v '+version_numbers[2]+'), csbdeep (v '+version_numbers[3]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
        "\n",
        "    if pretrained_model:\n",
        "        text = 'The '+Network+' model was trained for '+str(number_of_steps)+' steps on '+str(dataset_size)+' paired image patches (image dimensions: '+str(shape)+', patch_size: '+str(patch_size)+') with a batch size of '+str(batch_size)+' and a MSELoss loss function, using the '+Network+' ZeroCostDL4Mic notebook (v '+Notebook_version[0]+') (von Chamier & Laine et al., 2020). The model was retrained from a pretrained model. Key python packages used include tensorflow (v '+version_numbers[0]+'), keras (v '+version_numbers[2]+'), numpy (v '+version_numbers[1]+'), cuda (v '+cuda_version+'). The training was accelerated using a '+gpu_name+'GPU.'\n",
        "\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font_size(10.)\n",
        "    pdf.multi_cell(190, 5, txt = text, align='L')\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size = 10, style = 'B')\n",
        "    pdf.ln(1)\n",
        "    pdf.cell(28, 5, txt='Augmentation: ', ln=0)\n",
        "    pdf.set_font('')\n",
        "    if augmentation:\n",
        "        aug_text = 'The dataset was augmented ' + 'by'\n",
        "        if Flip:\n",
        "            aug_text += '\\n- flipping'\n",
        "        if Crop:\n",
        "            aug_text += '\\n- random crops'\n",
        "    else:\n",
        "        aug_text = 'No augmentation was used for training.'\n",
        "    pdf.multi_cell(190, 5, txt=aug_text, align='L')\n",
        "    pdf.set_font('Arial', size = 11, style = 'B')\n",
        "    pdf.ln(1)\n",
        "    pdf.cell(180, 5, txt = 'Parameters', align='L', ln=1)\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font_size(10.)\n",
        "    if Use_Default_Advanced_Parameters:\n",
        "        pdf.cell(200, 5, txt='Default Advanced Parameters were enabled')\n",
        "    pdf.cell(200, 5, txt='The following parameters were used for training:')\n",
        "    pdf.ln(1)\n",
        "    html = \"\"\"\n",
        "    <table width=60% style=\"margin-left:0px;\">\n",
        "      <tr>\n",
        "        <th width = 50% align=\"left\">Parameter</th>\n",
        "        <th width = 50% align=\"left\">Value</th>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>number_of_steps</td>\n",
        "        <td width = 50%>{0}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>batch_size</td>\n",
        "        <td width = 50%>{1}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>patch_size</td>\n",
        "        <td width = 50%>{2}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>random_seed</td>\n",
        "        <td width = 50%>{3}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>initial_learning_rate</td>\n",
        "        <td width = 50%>{4}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>valid_interval</td>\n",
        "        <td width = 50%>{5}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>ckpt_period</td>\n",
        "        <td width = 50%>{6}</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td width = 50%>percentage_validation</td>\n",
        "        <td width = 50%>{7}</td>\n",
        "      </tr>\n",
        "    </table>\n",
        "    \"\"\".format(number_of_steps, batch_size, patch_size, random_seed, initial_learning_rate, valid_interval, ckpt_period, percentage_validation)\n",
        "    pdf.write_html(html)\n",
        "\n",
        "    #pdf.multi_cell(190, 5, txt = text_2, align='L')\n",
        "    pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "    pdf.ln(1)\n",
        "    pdf.cell(190, 5, txt = 'Training Dataset', align='L', ln=1)\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size = 10, style = 'B')\n",
        "    pdf.cell(29, 5, txt= 'Training_source:', align = 'L', ln=0)\n",
        "    pdf.set_font('')\n",
        "    pdf.multi_cell(170, 5, txt = Training_source, align = 'L')\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size = 10, style = 'B')\n",
        "    pdf.cell(27, 5, txt= 'Training_target:', align = 'L', ln=0)\n",
        "    pdf.set_font('')\n",
        "    pdf.multi_cell(170, 5, txt = Training_target, align = 'L')\n",
        "    #pdf.cell(190, 5, txt=aug_text, align='L', ln=1)\n",
        "    pdf.ln(1)\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size = 10, style = 'B')\n",
        "    pdf.cell(22, 5, txt= 'Model Path:', align = 'L', ln=0)\n",
        "    pdf.set_font('')\n",
        "    pdf.multi_cell(170, 5, txt = model_path+'/'+model_name, align = 'L')\n",
        "    pdf.ln(1)\n",
        "    pdf.cell(60, 5, txt = 'Example Training pair', ln=1)\n",
        "    pdf.ln(1)\n",
        "    exp_size = io.imread(\"./TrainingExampleData_UNet3D.png\").shape\n",
        "    pdf.image(\"./TrainingExampleData_UNet3D.png\", x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
        "\n",
        "    pdf.ln(1)\n",
        "    ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
        "    pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
        "    pdf.ln(1)\n",
        "    ref_2 = '- Your networks name: first author et al. \"Title of publication\" Journal, year'\n",
        "    pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
        "#     if augmentation:\n",
        "#       ref_3 = '- Augmentor: Bloice, Marcus D., Christof Stocker, and Andreas Holzinger. \"Augmentor: an image augmentation library for machine learning.\" arXiv preprint arXiv:1708.04680 (2017).'\n",
        "#       pdf.multi_cell(190, 5, txt = ref_3, align='L')\n",
        "    pdf.ln(3)\n",
        "    reminder = 'Important:\\nRemember to perform the quality control step on all newly trained models\\nPlease consider depositing your training dataset on Zenodo'\n",
        "    pdf.set_font('Arial', size = 11, style='B')\n",
        "    pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
        "\n",
        "    pdf.output(model_path+'/'+model_name+'/'+model_name+\"_training_report.pdf\")\n",
        "\n",
        "    print('------------------------------')\n",
        "    print('PDF report exported in '+model_path+'/'+model_name+'/')\n",
        "\n",
        "\n",
        "#Make a pdf summary of the QC results\n",
        "\n",
        "def qc_pdf_export():\n",
        "    class MyFPDF(FPDF, HTMLMixin):\n",
        "        pass\n",
        "\n",
        "    pdf = MyFPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_right_margin(-1)\n",
        "    pdf.set_font(\"Arial\", size = 11, style='B')\n",
        "\n",
        "    Network = \"Calcium U-Net3D\"\n",
        "\n",
        "    day = datetime.now()\n",
        "    datetime_str = str(day)[0:10]\n",
        "\n",
        "    Header = 'Quality Control report for '+Network+' model ('+QC_model_name+')\\nDate: '+datetime_str\n",
        "    pdf.multi_cell(180, 5, txt = Header, align = 'L')\n",
        "    pdf.ln(1)\n",
        "\n",
        "    all_packages = ''\n",
        "    for requirement in freeze(local_only=True):\n",
        "        all_packages = all_packages+requirement+', '\n",
        "\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size = 11, style = 'B')\n",
        "    pdf.ln(2)\n",
        "    pdf.cell(190, 5, txt = 'Development of Training Losses', ln=1, align='L')\n",
        "    pdf.ln(1)\n",
        "    exp_size = io.imread(full_QC_model_path+'Quality Control/lossCurvePlots.png').shape\n",
        "    if os.path.exists(full_QC_model_path+'Quality Control/lossCurvePlots.png'):\n",
        "        pdf.image(full_QC_model_path+'Quality Control/lossCurvePlots.png', x = 11, y = None, w = round(exp_size[1]/8), h = round(exp_size[0]/8))\n",
        "    else:\n",
        "        pdf.set_font('')\n",
        "        pdf.set_font('Arial', size=10)\n",
        "        pdf.multi_cell(190, 5, txt='If you would like to see the evolution of the loss function during training please play the first cell of the QC section in the notebook.', align='L')\n",
        "\n",
        "    pdf.ln(2)\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size = 10, style = 'B')\n",
        "    pdf.ln(3)\n",
        "    pdf.cell(80, 5, txt = 'Example Quality Control Visualisation', ln=1)\n",
        "    pdf.ln(1)\n",
        "    exp_size = io.imread(full_QC_model_path+'Quality Control/QC_example_data.png').shape\n",
        "    pdf.image(full_QC_model_path+'Quality Control/QC_example_data.png', x = 16, y = None, w = round(exp_size[1]/10), h = round(exp_size[0]/10))\n",
        "    pdf.ln(1)\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font('Arial', size = 11, style = 'B')\n",
        "    pdf.ln(1)\n",
        "    pdf.cell(180, 5, txt = 'Quality Control Metrics', align='L', ln=1)\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font_size(10.)\n",
        "\n",
        "    pdf.ln(1)\n",
        "    html = \"\"\"\n",
        "    <body>\n",
        "    <font size=\"7\">\n",
        "    <table width=94% style=\"margin-left:0px;\">\"\"\"\n",
        "    with open(full_QC_model_path+'Quality Control/QC_metrics_'+QC_model_name+'.csv', 'r') as csvfile:\n",
        "        metrics = csv.reader(csvfile)\n",
        "        header = next(metrics)\n",
        "        image = header[0]\n",
        "        precision = header[1]\n",
        "        recall = header[2]\n",
        "        iou = header[3]\n",
        "        header = \"\"\"\n",
        "        <tr>\n",
        "          <th width = 10% align=\"left\">{0}</th>\n",
        "          <th width = 15% align=\"left\">{1}</th>\n",
        "          <th width = 15% align=\"center\">{2}</th>\n",
        "          <th width = 15% align=\"left\">{3}</th>\n",
        "        </tr>\"\"\".format(image, precision, recall, iou)\n",
        "        html = html+header\n",
        "        for row in metrics:\n",
        "            image = row[0]\n",
        "            precision = row[1]\n",
        "            recall = row[2]\n",
        "            iou = row[3]\n",
        "            cells = \"\"\"\n",
        "            <tr>\n",
        "              <td width = 10% align=\"left\">{0}</td>\n",
        "              <td width = 15% align=\"center\">{1}</td>\n",
        "              <td width = 15% align=\"center\">{2}</td>\n",
        "              <td width = 15% align=\"center\">{3}</td>\n",
        "            </tr>\"\"\".format(image,str(round(float(precision),3)),str(round(float(recall),3)),str(round(float(iou),3)))\n",
        "            html = html+cells\n",
        "    html = html+\"\"\"</table></font></body>\"\"\"\n",
        "\n",
        "    pdf.write_html(html)\n",
        "\n",
        "    pdf.ln(1)\n",
        "    pdf.set_font('')\n",
        "    pdf.set_font_size(10.)\n",
        "    ref_1 = 'References:\\n - ZeroCostDL4Mic: von Chamier, Lucas & Laine, Romain, et al. \"Democratising deep learning for microscopy with ZeroCostDL4Mic.\" Nature Communications (2021).'\n",
        "    pdf.multi_cell(190, 5, txt = ref_1, align='L')\n",
        "    pdf.ln(1)\n",
        "    ref_2 = '- Your networks name: first author et al. \"Title of publication\" Journal, year'\n",
        "    pdf.multi_cell(190, 5, txt = ref_2, align='L')\n",
        "\n",
        "    pdf.ln(3)\n",
        "    reminder = 'To find the parameters and other information about how this model was trained, go to the training_report.pdf of this model which should be in the folder of the same name.'\n",
        "\n",
        "    pdf.set_font('Arial', size = 11, style='B')\n",
        "    pdf.multi_cell(190, 5, txt=reminder, align='C')\n",
        "    pdf.output(full_QC_model_path+'Quality Control/'+QC_model_name+'_QC_report.pdf')\n",
        "\n",
        "print(\"Depencies installed and imported.\")\n",
        "\n",
        "# Build requirements file for local run\n",
        "# -- the developers should leave this below all the other installations\n",
        "after = [str(m) for m in sys.modules]\n",
        "# build_requirements_file(before, after)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kHuNfSDU2SyR"
      },
      "outputs": [],
      "source": [
        "#@markdown Optionaly, one can download a subset of training data\n",
        "download_path = \"/content/\"\n",
        "if not os.path.exists(download_path + \"subset-calcium-dataset.h5\"):\n",
        "    wget.download(\"https://s3.valeria.science/flclab-calcium/data/subset-calcium-dataset.h5\", download_path + \"subset-calcium-dataset.h5\", bar=bar_progress)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPOJkyFYiA15"
      },
      "source": [
        "# **2. Initialise the Colab session**\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dvLrwF_iEXS"
      },
      "source": [
        "\n",
        "## **2.1. Check for GPU access**\n",
        "---\n",
        "\n",
        "By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n",
        "\n",
        "<font size = 4>Go to **Runtime -> Change the Runtime type**\n",
        "\n",
        "<font size = 4>**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n",
        "\n",
        "<font size = 4>**Accelerator: GPU** *(Graphics processing unit)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8o_-wbDOiIHF"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Run this cell to check if you have GPU access\n",
        "\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name()=='':\n",
        "  print('You do not have GPU access.')\n",
        "  print('Did you change your runtime ?')\n",
        "  print('If the runtime settings are correct then Google did not allocate GPU to your session')\n",
        "  print('Expect slow performance. To access GPU try reconnecting later')\n",
        "\n",
        "else:\n",
        "  print('You have GPU access')\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEyJvvxSiN6L"
      },
      "source": [
        "## **2.2. Mount your Google Drive**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive, you need to mount your Google Drive to this notebook.\n",
        "\n",
        "<font size = 4> Play the cell below to mount your Google Drive and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WWVR1U5tiM9h"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Run this cell to connect your Google Drive to Colab\n",
        "\n",
        "#@markdown * Click on the URL.\n",
        "\n",
        "#@markdown * Sign in your Google Account.\n",
        "\n",
        "#@markdown * Copy the authorization code.\n",
        "\n",
        "#@markdown * Enter the authorization code.\n",
        "\n",
        "#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\".\n",
        "\n",
        "#mounts user's Google Drive to Google Colab.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKaeBnSuifZn"
      },
      "source": [
        "# **3. Select your paths and parameters**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>The code below allows the user to enter the paths to where the training data is and to define the training parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StTGluw2iidc"
      },
      "source": [
        "## **3.1. Setting the main training parameters**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyRjBdClimfK"
      },
      "source": [
        "<font size = 5> **Paths for training, predictions and results**\n",
        "\n",
        "<font size = 4>**`Training_source:`, `Training_target`:** These are the paths to your folders containing the Training_source and Training_target data respectively. To find the paths of the folders containing the respective datasets, go to your Files on the left of the notebook, navigate to the folder containing your files and copy the path by right-clicking on the folder, **Copy path** and pasting it into the right box below.\n",
        "\n",
        "<font size = 4>**`model_name`:** Use only my_model -style, not my-model (Use \"_\" not \"-\"). Do not use spaces in the name. Avoid using the name of an existing model (saved in the same folder) as it will be overwritten.\n",
        "\n",
        "<font size = 4>**`model_path`**: Enter the path where your model will be saved once trained (for instance your result folder).\n",
        "    \n",
        "<font size = 4>**`dataset_type`**: When a *hdf5* dataset is used, the dataset architecture can be selected from `normal`, `crop`. `normal`: events have already been extracted, `crop`: events will be extracted from full stream. **Default value: `crop`**\n",
        "\n",
        "<font size = 5>**Training parameters**\n",
        "\n",
        "* <font size = 4>**`number_of_steps`:** Give estimates for training performance given a number of steps and provide a default value. *Default value: 100*\n",
        "* <font size = 4>**`patch_size`:** Give the size of the patches used for training *Default value: 32*\n",
        "* <font size = 4>**`batch_size`:** Give the size of the batches used for training *Default value: 16*\n",
        "* <font size = 4>**`initial_learning_rate`:** Give the initial learning rate used for training *Default value: 0.0002*\n",
        "* <font size = 4>**`valid_interval`:** Give the number of steps before the validation of the model occurs *Default value: 100*\n",
        "* <font size = 4>**`ckpt_period`:** Give the number of steps before the state of the model is saved to disk *Default value: 100*  \n",
        "* <font size = 4>**`percentage_validation`:** Give the percentage of images used for validation. Only used when folder of images is used *Default value: 20*   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "i1sKnXrDieiR"
      },
      "outputs": [],
      "source": [
        "class bcolors:\n",
        "    WARNING = '\\033[31m'\n",
        "    NORMAL = '\\033[0m'  # white (normal)\n",
        "\n",
        "#@markdown ###Path to training images:\n",
        "Training_source = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# Ground truth images\n",
        "Training_target = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Name of the model and path to model folder:\n",
        "model_name = \"\" #@param {type:\"string\"}\n",
        "model_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Dataset management\n",
        "#@markdown The next options are only required if an hdf5 training source is used\n",
        "dataset_type = \"crop\" #@param [\"crop\", \"normal\"]\n",
        "\n",
        "# other parameters for training.\n",
        "#@markdown ### Training Parameters\n",
        "#@markdown Number of steps:\n",
        "number_of_steps = 100 #@param {type:\"number\"}\n",
        "\n",
        "#@markdown Other parameters, add as necessary\n",
        "patch_size = 32 #@param {type:\"number\"} # in pixels\n",
        "\n",
        "#@markdown ###Advanced Parameters\n",
        "\n",
        "Use_Default_Advanced_Parameters = True #@param {type:\"boolean\"}\n",
        "#@markdown ###If not, please input:\n",
        "\n",
        "random_seed = 42 #@param {type:\"number\"}\n",
        "batch_size =  16 #@param {type:\"number\"}\n",
        "initial_learning_rate =  0.0002 #@param {type:\"number\"}\n",
        "valid_interval = 50 #@param {type:\"number\"}\n",
        "ckpt_period = 50 #@param {type:\"number\"}\n",
        "percentage_validation = 20 #@param {type:\"number\"}\n",
        "\n",
        "if (Use_Default_Advanced_Parameters):\n",
        "    print(\"Default advanced parameters enabled\")\n",
        "    random_seed = 42\n",
        "    batch_size = 16\n",
        "    initial_learning_rate =  0.0002\n",
        "    valid_interval = 50\n",
        "    ckpt_period = 50\n",
        "    percentage_validation = 20\n",
        "\n",
        "#Here we define the percentage to use for validation\n",
        "percentage = percentage_validation/100\n",
        "\n",
        "print(\"Parameters initiated.\")\n",
        "\n",
        "# This will display a randomly chosen dataset input and output\n",
        "if os.path.isfile(Training_source):\n",
        "    with h5py.File(Training_source, \"r\") as file:\n",
        "        neuron_id = random.choice(list(file[\"train\"].keys()))\n",
        "        if dataset_type == \"normal\":\n",
        "            event_id = random.choice(list(file[\"train\"][neuron_id][\"input\"].keys()))\n",
        "            x = file[\"train\"][neuron_id][\"input\"][event_id][()]\n",
        "            y = file[\"train\"][neuron_id][\"label\"][event_id][()]\n",
        "        else:\n",
        "            x = file[\"train\"][neuron_id][\"input\"]\n",
        "            y = file[\"train\"][neuron_id][\"label\"]\n",
        "            event = random.choice(file[\"train\"][neuron_id][\"events\"])\n",
        "            event_id, event = event[0], event[1:]\n",
        "\n",
        "            event_center = event.reshape(-1, 2)\n",
        "            event_center = numpy.mean(event_center, axis=-1).astype(int)\n",
        "            slc = tuple(\n",
        "                slice(max(0, c - s // 2), min(_max, c + s // 2)) for c, s, _max in zip(event_center, [patch_size * 2] * x.ndim, x.shape)\n",
        "            )\n",
        "            x, y = x[slc], y[slc]\n",
        "\n",
        "        print(f\"Neuron ID: {neuron_id} --- Event ID: {event_id}\")\n",
        "\n",
        "else:\n",
        "    random_choice = random.choice(os.listdir(Training_source))\n",
        "    x = tifffile.imread(os.path.join(Training_source, random_choice))\n",
        "    y = tifffile.imread(os.path.join(Training_target, random_choice))\n",
        "\n",
        "# Here we check that the input images contains the expected dimensions\n",
        "if len(x.shape) == 3:\n",
        "    print(\"Image dimensions (y,x)\",x.shape)\n",
        "\n",
        "if not len(x.shape) == 3:\n",
        "    print(bcolors.WARNING +\"Your images appear to have the wrong dimensions. Image dimension\",x.shape)\n",
        "\n",
        "#Find image XY dimension\n",
        "Image_Z = x.shape[0]\n",
        "Image_Y = x.shape[1]\n",
        "Image_X = x.shape[2]\n",
        "\n",
        "#Hyperparameters failsafes\n",
        "# Here we check that patch_size is smaller than the smallest xy dimension of the image\n",
        "if patch_size > min(Image_Z, Image_Y, Image_X):\n",
        "    patch_size = min(Image_Y, Image_X)\n",
        "    print (bcolors.WARNING + \" Your chosen patch_size is bigger than the xy dimension of your image; therefore the patch_size chosen is now:\",patch_size)\n",
        "\n",
        "# Here we check that patch_size is divisible by 8\n",
        "if not patch_size % 8 == 0:\n",
        "    patch_size = ((int(patch_size / 8)-1) * 8)\n",
        "    print (bcolors.WARNING + \" Your chosen patch_size is not divisible by 8; therefore the patch_size chosen is now:\",patch_size)\n",
        "\n",
        "# Sets training seed\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "numpy.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "\n",
        "config = {\n",
        "  \"model_config\": {\n",
        "    \"first_kernel_size\": 3,\n",
        "    \"inC\": 1,\n",
        "    \"n_layer\": 5,\n",
        "    \"nbf\": 8,\n",
        "    \"outC\": 1,\n",
        "    \"r\": 1,\n",
        "    \"use_batch_norm\": True,\n",
        "    \"use_leaky_relu\": True\n",
        "  },\n",
        "  \"training_config\": {\n",
        "    \"batch_size\": batch_size,\n",
        "    \"drop_last\": False,\n",
        "    \"inference_batch_size\": 4,\n",
        "    \"num_steps\": number_of_steps,\n",
        "    \"num_workers\": os.cpu_count() - 1,\n",
        "    \"optimizer\": {\n",
        "      \"lr\": initial_learning_rate\n",
        "    },\n",
        "    \"valid_interval\": valid_interval,\n",
        "    \"valid_proportion\": 0.2,\n",
        "  }\n",
        "}\n",
        "model = UNet3D(config)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "def scroll_in_z(z):\n",
        "    f=pyplot.figure(figsize=(16,8))\n",
        "    pyplot.subplot(1,2,1)\n",
        "    pyplot.imshow(x[z-1, :, :], cmap='gray', vmin=0, vmax=x.max())\n",
        "    pyplot.title('Sample augmented source (z = ' + str(z) + ')', fontsize=15)\n",
        "    pyplot.axis('off')\n",
        "\n",
        "    pyplot.subplot(1,2,2)\n",
        "    pyplot.imshow(y[z-1, :, :], cmap='gray')\n",
        "    pyplot.title('Sample training target (z = ' + str(z) + ')', fontsize=15)\n",
        "    pyplot.axis('off')\n",
        "\n",
        "    pyplot.savefig('./TrainingExampleData_UNet3D.png',bbox_inches='tight',pad_inches=0)\n",
        "    pyplot.show()\n",
        "\n",
        "print('This is what the augmented training images will look like with the chosen settings')\n",
        "interact(scroll_in_z, z=widgets.IntSlider(min=1, max=x.shape[0], step=1, value=0));\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLYZQA6GitQL"
      },
      "source": [
        "## **3.2. Data augmentation**\n",
        "---\n",
        "<font size = 4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4GfK6-1iwbf"
      },
      "source": [
        "<font size = 4>Data augmentation can improve training progress by amplifying differences in the dataset. This can be useful if the available dataset is small since, in this case, it is possible that a network could quickly learn every example in the dataset (overfitting), without augmentation. Augmentation is not necessary for training and if your training dataset is large you should disable it.\n",
        "\n",
        "<font size = 4>Data augmentation is performed here by flipping the patches in XY-Plane and applying random crops to the events. In validation, the event is assumed to be centered in the extracted patch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EkBGtraZi3Ob",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "augmentations = []\n",
        "\n",
        "Use_Data_augmentation = False #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown Select this option if you want to use augmentation to increase the size of your dataset\n",
        "\n",
        "#@markdown **Flip each image once around the x axis of the stack.**\n",
        "Flip = True #@param{type:\"boolean\"}\n",
        "if Flip:\n",
        "    augmentations.append(RandomFlip())\n",
        "\n",
        "Crop = True #@param{type:\"boolean\"}\n",
        "if Crop:\n",
        "    augmentations.append(RandomCrop(patch_size))\n",
        "else:\n",
        "    augmentations.append(CenterCrop(patch_size))\n",
        "\n",
        "if os.path.isfile(Training_source):\n",
        "    train_data = MSCTDataset(Training_source, [\"train\"], cache_mode=dataset_type)\n",
        "    valid_data = MSCTDataset(Training_source, [\"valid\"], cache_mode=dataset_type)\n",
        "else:\n",
        "    train_valid_data = MSCTDatasetFromFolder(Training_source, Training_target)\n",
        "\n",
        "    # Split train/valid\n",
        "    numpy.random.seed(random_seed)\n",
        "    indices = numpy.arange(len(train_valid_data))\n",
        "    indices.shuffle()\n",
        "    length = percentage * len(indices)\n",
        "\n",
        "    train_data = Subset(train_valid_data, indices[length:])\n",
        "    valid_data = Subset(train_valid_data, indices[:length])\n",
        "\n",
        "train_generator = LoadedEvent3D(train_data, transforms=Compose(augmentations))\n",
        "valid_generator = LoadedEvent3D(valid_data, transforms=CenterCrop(patch_size))\n",
        "\n",
        "if Use_Data_augmentation:\n",
        "    print('Data augmentation enabled.')\n",
        "    sample_src_aug, sample_tgt_aug, _ = train_generator[random.randint(0, len(train_generator) - 1)]\n",
        "\n",
        "    def scroll_in_z(z):\n",
        "        f=pyplot.figure(figsize=(16,8))\n",
        "        pyplot.subplot(1,2,1)\n",
        "        pyplot.imshow(sample_src_aug[0, z-1, :, :], cmap='gray', vmin=0, vmax=sample_src_aug.max())\n",
        "        pyplot.title('Sample augmented source (z = ' + str(z) + ')', fontsize=15)\n",
        "        pyplot.axis('off')\n",
        "\n",
        "        pyplot.subplot(1,2,2)\n",
        "        pyplot.imshow(sample_tgt_aug[0, z-1, :, :], cmap='gray')\n",
        "        pyplot.title('Sample training target (z = ' + str(z) + ')', fontsize=15)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.show()\n",
        "\n",
        "    print('This is what the augmented training images will look like with the chosen settings')\n",
        "    interact(scroll_in_z, z=widgets.IntSlider(min=1, max=sample_src_aug.shape[3], step=1, value=0));\n",
        "\n",
        "else:\n",
        "    print(bcolors.WARNING+\"Data augmentation disabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y-47ZmFiyG_"
      },
      "source": [
        "## **3.3. Using weights from a pre-trained model as initial weights**\n",
        "---\n",
        "<font size = 4>  Here, you can set the the path to a pre-trained model from which the weights can be extracted and used as a starting point for this training session. **This pre-trained model needs to be a model of Calcium U-Net 3D**.\n",
        "\n",
        "<font size = 4> This option allows you to perform training over multiple Colab runtimes or to do transfer learning using models trained outside of ZeroCostDL4Mic. **You do not need to run this section if you want to train a network from scratch**.\n",
        "\n",
        "<font size = 4> In order to continue training from the point where the pret-trained model left off, it is adviseable to also **load the learning rate** that was used when the training ended. This is automatically saved for models trained with ZeroCostDL4Mic and will be loaded here. If no learning rate can be found in the model folder provided, the default learning rate will be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jSb9luhrjHe-"
      },
      "outputs": [],
      "source": [
        "# @markdown ##Loading weights from a pre-trained network\n",
        "\n",
        "Use_pretrained_model = False #@param {type:\"boolean\"}\n",
        "pretrained_model_choice = \"Model_from_file\" #@param [\"Model_from_file\", \"pretrained-unet3d\"]\n",
        "Weights_choice = \"best\" #@param [\"checkpoint\", \"best\"]\n",
        "if Weights_choice == \"best\":\n",
        "    Weights_choice = \"results\"\n",
        "\n",
        "# @markdown ###If you chose \"Model_from_file\", please provide the path to the model folder:\n",
        "pretrained_model_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# --------------------- Check if we load a previously trained model ------------------------\n",
        "if Use_pretrained_model:\n",
        "\n",
        "# --------------------- Load the model from the choosen path ------------------------\n",
        "    if pretrained_model_choice == \"Model_from_file\":\n",
        "\n",
        "        checkpoint_path = os.path.join(pretrained_model_path, Weights_choice+\".pt\")\n",
        "\n",
        "\n",
        "# --------------------- Download the a model provided in the XXX ------------------------\n",
        "\n",
        "    if pretrained_model_choice == \"pretrained-unet3d\":\n",
        "        pretrained_model_name = \"pretrained-unet3d\"\n",
        "        pretrained_model_path = \"/content/\"+pretrained_model_name\n",
        "        print(\"Downloading the Pretrained U-Net 3D\")\n",
        "        if os.path.exists(pretrained_model_path):\n",
        "            shutil.rmtree(pretrained_model_path)\n",
        "        os.makedirs(pretrained_model_path)\n",
        "\n",
        "        if not os.path.exists(os.path.join(pretrained_model_path, 'unet3d-model.zip')):\n",
        "            wget.download(\"https://s3.valeria.science/flclab-calcium/data/unet3d-model.zip\", pretrained_model_path, bar=bar_progress)\n",
        "\n",
        "        # unzip\n",
        "        print(\"\\nUnzipping the Pretrained U-Net 3D\")\n",
        "        !unzip -q $pretrained_model_path/unet3d-model.zip -d $pretrained_model_path\n",
        "\n",
        "        pretrained_model_path = os.path.join(pretrained_model_path, \"unet3D-ZeroCostDL4Mic_subset-0.25-1_1-64_46\")\n",
        "        checkpoint_path = os.path.join(pretrained_model_path, Weights_choice+\".pt\")\n",
        "\n",
        "# --------------------- Add additional pre-trained models here ------------------------\n",
        "\n",
        "# --------------------- Check the model exist ------------------------\n",
        "# If the model path chosen does not contain a pretrain model then use_pretrained_model is disabled,\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(bcolors.WARNING+f'WARNING: {checkpoint_path} pretrained model does not exist')\n",
        "        Use_pretrained_model = False\n",
        "\n",
        "# If the model path contains a pretrain model, we load the training rate,\n",
        "    if os.path.exists(checkpoint_path):\n",
        "#Here we check if the learning rate can be loaded from the quality control folder\n",
        "        if os.path.exists(os.path.join(pretrained_model_path, 'Quality Control', 'train-stats.csv')):\n",
        "\n",
        "            with open(os.path.join(pretrained_model_path, 'Quality Control', 'train-stats.csv'),'r') as csvfile:\n",
        "                csvRead = pandas.read_csv(csvfile, sep=',')\n",
        "                #print(csvRead)\n",
        "\n",
        "            if \"learning rate\" in csvRead.columns: #Here we check that the learning rate column exist (compatibility with model trained un ZeroCostDL4Mic bellow 1.4)\n",
        "                print(\"pretrained network learning rate found\")\n",
        "                #find the last learning rate\n",
        "                lastLearningRate = csvRead[\"learning rate\"].iloc[-1]\n",
        "                #Find the learning rate corresponding to the lowest validation loss\n",
        "                min_val_loss = csvRead[csvRead['val_loss'] == min(csvRead['val_loss'])]\n",
        "                #print(min_val_loss)\n",
        "                bestLearningRate = min_val_loss['learning rate'].iloc[-1]\n",
        "\n",
        "            if Weights_choice == \"last\":\n",
        "                print('Last learning rate: '+str(lastLearningRate))\n",
        "\n",
        "            if Weights_choice == \"best\":\n",
        "                print('Learning rate of best validation loss: '+str(bestLearningRate))\n",
        "\n",
        "            if not \"learning rate\" in csvRead.columns: #if the column does not exist, then initial learning rate is used instead\n",
        "                if os.path.exists(os.path.join(pretrained_model_path, \"config.yaml\")):\n",
        "                    with open(os.path.join(pretrained_model_path, \"config.yaml\"), \"r\") as file:\n",
        "                        config = yaml.load(file, Loader=yaml.Loader)\n",
        "                    bestLearningRate = config[\"training_config\"][\"optimizer\"][\"lr\"]\n",
        "                    lastLearningRate = config[\"training_config\"][\"optimizer\"][\"lr\"]\n",
        "                else:\n",
        "                    bestLearningRate = initial_learning_rate\n",
        "                    lastLearningRate = initial_learning_rate\n",
        "                    print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(bestLearningRate)+' will be used instead' + bcolors.NORMAL)\n",
        "\n",
        "#Compatibility with models trained outside ZeroCostDL4Mic but default learning rate will be used\n",
        "        if not os.path.exists(os.path.join(pretrained_model_path, 'Quality Control', 'train-stats.csv')):\n",
        "            if os.path.exists(os.path.join(pretrained_model_path, \"config.yaml\")):\n",
        "                with open(os.path.join(pretrained_model_path, \"config.yaml\"), \"r\") as file:\n",
        "                    config = yaml.load(file, Loader=yaml.Loader)\n",
        "                bestLearningRate = config[\"training_config\"][\"optimizer\"][\"lr\"]\n",
        "                lastLearningRate = config[\"training_config\"][\"optimizer\"][\"lr\"]\n",
        "            else:\n",
        "                bestLearningRate = initial_learning_rate\n",
        "                lastLearningRate = initial_learning_rate\n",
        "                print(bcolors.WARNING+'WARNING: The learning rate cannot be identified from the pretrained network. Default learning rate of '+str(bestLearningRate)+' will be used instead' + bcolors.NORMAL)\n",
        "\n",
        "\n",
        "# Display info about the pretrained model to be loaded (or not)\n",
        "if Use_pretrained_model:\n",
        "    print('Weights found in:')\n",
        "    print(checkpoint_path)\n",
        "    print('will be loaded prior to training.')\n",
        "\n",
        "else:\n",
        "    checkpoint_path = None\n",
        "    print(bcolors.WARNING+'No pretrained nerwork will be used.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjTtP2OmjMqM"
      },
      "source": [
        "# **4. Train the network**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ9NgI6XjQIk"
      },
      "source": [
        "## **4.1. Train the network**\n",
        "---\n",
        "<font size = 4>When playing the cell below you should see updates after each epoch (round). Network training can take some time.\n",
        "\n",
        "<font size = 4>* **CRITICAL NOTE:** Google Colab has a time limit for processing (to prevent using GPU power for datamining). Training time must be less than 12 hours! If training takes longer than 12 hours, please decrease the number of epochs or number of patches.\n",
        "\n",
        "<font size = 4>Once training is complete, the trained model is automatically saved on your Google Drive, in the **model_path** folder that was selected in Section 3. It is however wise to download the folder as all data can be erased at the next training if using the same folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SVUd0Lr0jUjy",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import csv\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "#@markdown ## Start training\n",
        "\n",
        "# @markdown To resume training the current model\n",
        "resume_training = False # @param {type:\"boolean\"}\n",
        "full_model_path = os.path.join(model_path, model_name)\n",
        "#here we check that no model with the same name already exist, if so delete\n",
        "if not resume_training and os.path.exists(full_model_path):\n",
        "    shutil.rmtree(full_model_path)\n",
        "    print(bcolors.WARNING+'!! WARNING: Folder already exists and has been overwritten !!'+bcolors.NORMAL)\n",
        "\n",
        "if not os.path.exists(full_model_path):\n",
        "    os.makedirs(full_model_path)\n",
        "\n",
        "# Export the training parameters as pdf (before training, in case training fails)\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    pdf_export(augmentation = Use_Data_augmentation, pretrained_model = Use_pretrained_model)\n",
        "\n",
        "# Start Training\n",
        "\n",
        "# Sets training seed again to ensure reproducibility\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "numpy.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "\n",
        "#Insert the code necessary to initiate training of your model\n",
        "model.train_model(\n",
        "    train_generator,\n",
        "    valid_generator,\n",
        "    model_path = model_path,\n",
        "    model_name = model_name,\n",
        "    ckpt_path = checkpoint_path,\n",
        "    save_best_ckpt_only = False,\n",
        "    ckpt_period = ckpt_period,\n",
        "    resume_training = resume_training\n",
        ")\n",
        "\n",
        "#Note that the notebook should load weights either from the model that is\n",
        "#trained from scratch or if the pretrained weights are used (3.3.)\n",
        "\n",
        "# Displaying the time elapsed for training\n",
        "dt = time.time() - start\n",
        "mins, sec = divmod(dt, 60)\n",
        "hour, mins = divmod(mins, 60)\n",
        "print(\"Time elapsed:\",hour, \"hour(s)\",mins,\"min(s)\",round(sec),\"sec(s)\")\n",
        "\n",
        "# Export the training parameters as pdf (after training)\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    pdf_export(trained = True, augmentation = Use_Data_augmentation, pretrained_model = Use_pretrained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tm3aimXjZ1B"
      },
      "source": [
        "# **5. Evaluate your model**\n",
        "---\n",
        "\n",
        "<font size = 4>This section allows the user to perform important quality checks on the validity and generalisability of the trained model.\n",
        "\n",
        "<font size = 4>**We highly recommend to perform quality control on all newly trained models.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "QAXu1FR0jYZC"
      },
      "outputs": [],
      "source": [
        "# model name and path\n",
        "#@markdown ###Do you want to assess the model you just trained ?\n",
        "Use_the_current_trained_model = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ###If not, please provide the name of the model and path to model folder:\n",
        "#@markdown #####During training, the model files are automatically saved inside a folder named after model_name in section 3. Provide the path to this folder below.\n",
        "QC_model_folder = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#Here we define the loaded model name and path\n",
        "QC_model_name = os.path.basename(QC_model_folder)\n",
        "QC_model_path = os.path.dirname(QC_model_folder)\n",
        "\n",
        "if (Use_the_current_trained_model):\n",
        "    QC_model_name = model_name\n",
        "    QC_model_path = model_path\n",
        "\n",
        "full_QC_model_path = QC_model_path+'/'+QC_model_name+'/'\n",
        "if os.path.exists(full_QC_model_path):\n",
        "    print(\"The \"+QC_model_name+\" network will be evaluated\")\n",
        "\n",
        "    if not Use_the_current_trained_model:\n",
        "        # We load another model\n",
        "        model = UNet3D.from_path(QC_model_path, QC_model_name)\n",
        "        model = model.to(DEVICE)\n",
        "else:\n",
        "    W  = '\\033[0m'  # white (normal)\n",
        "    R  = '\\033[31m' # red\n",
        "    print(R+'!! WARNING: The chosen model does not exist !!'+W)\n",
        "    print('Please make sure you provide a valid model path and model name before proceeding further.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULMuc37njkXM"
      },
      "source": [
        "## **5.1. Inspection of the loss function**\n",
        "---\n",
        "\n",
        "<font size = 4>First, it is good practice to evaluate the training progress by comparing the training loss with the validation loss. The latter is a metric which shows how well the network performs on a subset of unseen data which is set aside from the training dataset. For more information on this, see for example [this review](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6381354/) by Nichols *et al.*\n",
        "\n",
        "<font size = 4>**Training loss** describes an error value after each epoch for the difference between the model's prediction and its ground-truth target.\n",
        "\n",
        "<font size = 4>**Validation loss** describes the same error value between the model's prediction on a validation image and compared to it's target.\n",
        "\n",
        "<font size = 4>During training both values should decrease before reaching a minimal value which does not decrease further even after more training. Comparing the development of the validation loss with the training loss can give insights into the model's performance.\n",
        "\n",
        "<font size = 4>Decreasing **Training loss** and **Validation loss** indicates that training is still necessary and increasing the `number_of_epochs` is recommended. Note that the curves can look flat towards the right side, just because of the y-axis scaling. The network has reached convergence once the curves flatten out. After this point no further training is required. If the **Validation loss** suddenly increases again an the **Training loss** simultaneously goes towards zero, it means that the network is overfitting to the training data. In other words the network is remembering the exact patterns from the training data and no longer generalizes well to unseen data. In this case the training dataset has to be increased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1VCvEofKjjHN"
      },
      "outputs": [],
      "source": [
        "#@markdown ##Play the cell to show a plot of training errors vs. epoch number\n",
        "import csv\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "with open(QC_model_path+'/'+QC_model_name+'/Quality Control/train-stats.csv','r') as csvfile:\n",
        "    df = pandas.read_csv(csvfile, delimiter=',')\n",
        "    stepsDataFromCSV = df[\"step\"]\n",
        "    lossDataFromCSV = df[\"loss\"]\n",
        "#     next(csvRead)\n",
        "#     for row in csvRead:\n",
        "#         lossDataFromCSV.append(float(row[0]))\n",
        "#         vallossDataFromCSV.append(float(row[1]))\n",
        "\n",
        "with open(QC_model_path+'/'+QC_model_name+'/Quality Control/valid-stats.csv','r') as csvfile:\n",
        "    df = pandas.read_csv(csvfile, delimiter=',')\n",
        "    valstepsDataFromCSV = df[\"step\"]\n",
        "    vallossDataFromCSV = df[\"loss\"]\n",
        "    vallossstdDataFromCSV = df[\"std-loss\"]\n",
        "\n",
        "pyplot.figure(figsize=(15,10))\n",
        "\n",
        "pyplot.subplot(2,1,1)\n",
        "pyplot.plot(stepsDataFromCSV,lossDataFromCSV, label='Training loss')\n",
        "pyplot.plot(valstepsDataFromCSV,vallossDataFromCSV, label='Validation loss')\n",
        "pyplot.fill_between(\n",
        "    valstepsDataFromCSV, vallossDataFromCSV - vallossstdDataFromCSV, vallossDataFromCSV + vallossstdDataFromCSV,\n",
        "    color=\"tab:orange\", alpha=0.3\n",
        ")\n",
        "pyplot.title('Training loss and validation loss vs. epoch number (linear scale)')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.xlabel('Steps')\n",
        "pyplot.legend()\n",
        "\n",
        "pyplot.subplot(2,1,2)\n",
        "pyplot.semilogy(stepsDataFromCSV,lossDataFromCSV, label='Training loss')\n",
        "pyplot.semilogy(valstepsDataFromCSV,vallossDataFromCSV, label='Validation loss')\n",
        "pyplot.fill_between(\n",
        "    valstepsDataFromCSV, vallossDataFromCSV - vallossstdDataFromCSV, vallossDataFromCSV + vallossstdDataFromCSV,\n",
        "    color=\"tab:orange\", alpha=0.3\n",
        ")\n",
        "pyplot.title('Training loss and validation loss vs. epoch number (log scale)')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.xlabel('Steps')\n",
        "pyplot.legend()\n",
        "pyplot.savefig(QC_model_path+'/'+QC_model_name+'/Quality Control/lossCurvePlots.png')\n",
        "pyplot.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1DAGwUjMg2n"
      },
      "source": [
        "## **5.2. Threshold estimation**\n",
        "---\n",
        "\n",
        "<font size = 4>This section will estimate the threshold to use based on the validation dataset of the model !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2gBE6PU0Mg2n",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#@markdown ##Choose the folders that contain your Quality Control dataset\n",
        "\n",
        "Source_QC_folder = \"\" #@param{type:\"string\"}\n",
        "Target_QC_folder = \"\" #@param{type:\"string\"}\n",
        "\n",
        "if os.path.isfile(Source_QC_folder):\n",
        "    valid_data = MSCTDataset(Source_QC_folder, [\"valid\"], max_cache_size=0, cache_mode=dataset_type)\n",
        "else:\n",
        "    train_valid_data = MSCTDatasetFromFolder(Source_QC_folder, Target_QC_folder)\n",
        "\n",
        "    # Split train/valid\n",
        "    numpy.random.seed(random_seed)\n",
        "    indices = numpy.arange(len(train_valid_data))\n",
        "    indices.shuffle()\n",
        "    length = percentage * len(indices)\n",
        "\n",
        "    valid_data = Subset(train_valid_data, indices[:length])\n",
        "\n",
        "threshold = model.optimize_threshold(valid_generator)\n",
        "with open(os.path.join(QC_model_path, QC_model_name, \"optimized-threshold\"), \"w\") as file:\n",
        "    file.write(str(threshold))\n",
        "\n",
        "print(f\"The optimal threshold is: {threshold:0.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smiWe2wcjwTc"
      },
      "source": [
        "## **5.3. Error mapping and quality metrics estimation**\n",
        "---\n",
        "\n",
        "<font size = 4>This section will display metrics for all the images provided in the \"Source_QC_folder\" and \"Target_QC_folder\" !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BUwOx2e6AhTu"
      },
      "outputs": [],
      "source": [
        "#@markdown Optionaly, one can download a subset of testing data\n",
        "download_path = \"/content/\"\n",
        "if not os.path.exists(download_path + \"subset-testing-dataset.zip\"):\n",
        "    wget.download(\"https://s3.valeria.science/flclab-calcium/data/subset-testing-dataset.zip\", download_path + \"subset-testing-dataset.zip\", bar=bar_progress)\n",
        "    !unzip -q {download_path}subset-testing-dataset.zip -d $download_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z179Zxgtj0PP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#@markdown ##Choose the folders that contain your Quality Control dataset\n",
        "\n",
        "Source_QC_folder = \"\" #@param{type:\"string\"}\n",
        "Target_QC_folder = \"\" #@param{type:\"string\"}\n",
        "\n",
        "# Create a quality control/Prediction Folder\n",
        "if os.path.exists(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/Prediction\"):\n",
        "    shutil.rmtree(QC_model_path+\"/\"+QC_model_name+\"/Quality Control/Prediction\")\n",
        "\n",
        "quality_control_prediction = QC_model_path+\"/\"+QC_model_name+\"/Quality Control/Prediction\"\n",
        "os.makedirs(quality_control_prediction)\n",
        "\n",
        "# List Tif images in Source_QC_folder\n",
        "Source_QC_folder_tif = Source_QC_folder+\"/*.tif\"\n",
        "Z = sorted(glob.glob(Source_QC_folder_tif))\n",
        "target_Z = [file.replace(Source_QC_folder, Target_QC_folder) for file in Z]\n",
        "target_Z = []\n",
        "for file in Z:\n",
        "    target_file = file.replace(Source_QC_folder, Target_QC_folder)\n",
        "    dirname, basename = os.path.dirname(target_file), os.path.basename(target_file)\n",
        "    name, ext = os.path.splitext(basename)\n",
        "    target_Z.append(os.path.join(dirname, \"\".join((\"manual_\", name, \".csv\"))))\n",
        "\n",
        "POSTPROCESS_PARAMS = {\n",
        "    \"minimal_time\" : 2,\n",
        "    \"minimal_height\" : 3,\n",
        "    \"minimal_width\" : 3\n",
        "}\n",
        "\n",
        "# Z = list(map(imread,Z))\n",
        "print('Number of test dataset found in the folder: '+str(len(Z)))\n",
        "\n",
        "# Loads threshold from folder\n",
        "with open(os.path.join(QC_model_path, QC_model_name, \"optimized-threshold\"), \"r\") as file:\n",
        "    threshold = float(file.read())\n",
        "print(f\"Loaded threshold from file: {threshold:0.3f}\")\n",
        "\n",
        "# Insert code to perform predictions on all datasets in the Source_QC folder\n",
        "out = {}\n",
        "for source_file, target_file in zip(tqdm(Z, desc=\"Images\"), target_Z):\n",
        "\n",
        "    stream_raw = tifffile.imread(source_file)\n",
        "    if stream_raw.ndim != 3:\n",
        "        print(\"[!!!!] File does not appear to be a stream\")\n",
        "        print(f\"[----] {source_file}\")\n",
        "        continue\n",
        "    stream = preprocess_stream(stream_raw)\n",
        "\n",
        "    prediction = model.predict_stream(\n",
        "        {\"input\" : stream, \"shape2crop\" : numpy.array([patch_size * 2, patch_size * 2, patch_size * 2])},\n",
        "        batch_size = batch_size * 2,\n",
        "        step = (patch_size, patch_size, patch_size),\n",
        "        num_workers = 0,\n",
        "        device = DEVICE\n",
        "    )\n",
        "\n",
        "    binary_prediction = (prediction > threshold).astype(int)\n",
        "\n",
        "    label = measure.label(binary_prediction)\n",
        "    regionprops = measure.regionprops(label, intensity_image=stream)\n",
        "\n",
        "    updated_regionprops = filter_regionprops(regionprops, POSTPROCESS_PARAMS)\n",
        "\n",
        "    print(\"Calculating metrics...\")\n",
        "\n",
        "    out[source_file] = {}\n",
        "\n",
        "    if os.path.isfile(target_file):\n",
        "        truth_coords = pandas.read_csv(target_file)[[\"Slice\", \"Y\", \"X\"]].to_numpy()\n",
        "        pred_coords = [rprop.weighted_centroid for rprop in updated_regionprops]\n",
        "        detector = CentroidDetectionError(truth_coords, pred_coords, algorithm=\"hungarian\", threshold=6)\n",
        "\n",
        "        out[source_file][\"det-precision\"] = detector.precision\n",
        "        out[source_file][\"det-recall\"] = detector.recall\n",
        "        out[source_file][\"det-f1-score\"] = detector.f1_score\n",
        "    else:\n",
        "        out[source_file][\"det-precision\"] = -1\n",
        "        out[source_file][\"det-recall\"] = -1\n",
        "        out[source_file][\"det-f1-score\"] = -1\n",
        "\n",
        "    print(\"Saving prediction...\")\n",
        "    savename = os.path.splitext(os.path.basename(source_file))[0] + '_prediction.tif'\n",
        "    tifffile.imwrite(\n",
        "        os.path.join(quality_control_prediction, savename), (binary_prediction * 255).astype(numpy.uint8)\n",
        "    )\n",
        "\n",
        "    # Saves Quality Control metrics\n",
        "    df = pandas.DataFrame.from_dict(out, orient='index')\n",
        "    df.to_csv(os.path.join(QC_model_path, QC_model_name, 'Quality Control/QC_metrics_'+QC_model_name+'.csv'))\n",
        "\n",
        "    del stream_raw, stream, prediction, binary_prediction, label, regionprops, updated_regionprops\n",
        "\n",
        "print(\"Predicted images!\")\n",
        "display(df)\n",
        "\n",
        "predicted_files = glob.glob(quality_control_prediction+\"/*.tif\")\n",
        "test_prediction = random.choice(predicted_files)\n",
        "test_source = test_prediction.replace(quality_control_prediction, Source_QC_folder).replace(\"_prediction.tif\", \".tif\")\n",
        "\n",
        "test_source = tifffile.imread(test_source)\n",
        "test_prediction = tifffile.imread(test_prediction)\n",
        "\n",
        "def scroll_in_z(z):\n",
        "\n",
        "    pyplot.figure(figsize=(25,5))\n",
        "    # Source\n",
        "    pyplot.subplot(1,3,1)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(test_source[z-1], cmap='gray', vmin=test_source.min(), vmax=0.35 * test_source.max())\n",
        "    pyplot.title('Source (z = ' + str(z) + ')', fontsize=15)\n",
        "\n",
        "#     # Target (Ground-truth)\n",
        "#     plt.subplot(1,4,2)\n",
        "#     plt.axis('off')\n",
        "#     plt.imshow(test_target[z-1], cmap='gray')\n",
        "#     plt.title('Target (z = ' + str(z) + ')', fontsize=15)\n",
        "\n",
        "    # Prediction\n",
        "    pyplot.subplot(1,3,2)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(test_prediction[z-1], cmap='magma', vmin=0, vmax=1)\n",
        "    pyplot.title('Prediction (z = ' + str(z) + ')', fontsize=15)\n",
        "\n",
        "    # Overlay\n",
        "    pyplot.subplot(1,3,3)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(test_source[z-1], cmap='gray', vmin=test_source.min(), vmax=0.35 * test_source.max())\n",
        "    pyplot.imshow(test_prediction[z-1], alpha=0.5, cmap='magma', vmin=0, vmax=1)\n",
        "    pyplot.title('Overlay (z = ' + str(z) + ')', fontsize=15)\n",
        "    pyplot.savefig(os.path.join(QC_model_path,QC_model_name,'Quality Control')+'/QC_example_data.png', bbox_inches='tight', pad_inches=0)\n",
        "    pyplot.show()\n",
        "\n",
        "interact(scroll_in_z, z=widgets.IntSlider(min=1, max=test_source.shape[0], step=1, value=0));\n",
        "\n",
        "#Make a pdf summary of the QC results\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    qc_pdf_export()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB8QNLekkCyZ"
      },
      "source": [
        "# **6. Using the trained model**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>In this section the unseen data is processed using the trained model (in section 4). First, your unseen images are uploaded and prepared for prediction. After that your trained model from section 4 is activated and finally saved into your Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2DrAOANkIWu"
      },
      "source": [
        "## **6.1. Generate prediction(s) from unseen dataset**\n",
        "---\n",
        "\n",
        "<font size = 4>The current trained model (from section 4.2) can now be used to process images. If you want to use an older model, untick the **Use_the_current_trained_model** box and enter the name and path of the model to use. Predicted output images are saved in your **Result_folder** folder as restored image stacks (ImageJ-compatible TIFF images).\n",
        "\n",
        "<font size = 4>**`Data_folder`:** This folder should contain the images that you want to use your trained network on for processing.\n",
        "\n",
        "<font size = 4>**`Result_folder`:** This folder will contain the predicted output images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mELG8z-ykCKV"
      },
      "outputs": [],
      "source": [
        "#@markdown ### Provide the path to your dataset and to the folder where the predictions are saved, then play the cell to predict outputs from your unseen images.\n",
        "\n",
        "Data_folder = \"\" #@param {type:\"string\"}\n",
        "Result_folder = \"\" #@param {type:\"string\"}\n",
        "\n",
        "# model name and path\n",
        "#@markdown ###Do you want to use the current trained model?\n",
        "Use_the_current_trained_model = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ###If not, provide the name of the model and path to model folder:\n",
        "#@markdown #####During training, the model files are automatically saved inside a folder named after model_name in section 3. Provide the path to this folder below.\n",
        "Prediction_model_folder = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#Here we find the loaded model name and parent path\n",
        "Prediction_model_name = os.path.basename(Prediction_model_folder)\n",
        "Prediction_model_path = os.path.dirname(Prediction_model_folder)\n",
        "\n",
        "if (Use_the_current_trained_model):\n",
        "    print(\"Using current trained network\")\n",
        "    Prediction_model_name = model_name\n",
        "    Prediction_model_path = model_path\n",
        "\n",
        "patch_size=32\n",
        "batch_size=64\n",
        "\n",
        "full_Prediction_model_path = Prediction_model_path+'/'+Prediction_model_name+'/'\n",
        "if os.path.exists(full_Prediction_model_path):\n",
        "    print(\"The \"+Prediction_model_name+\" network will be used.\")\n",
        "else:\n",
        "    W  = '\\033[0m'  # white (normal)\n",
        "    R  = '\\033[31m' # red\n",
        "    print(R+'!! WARNING: The chosen model does not exist !!'+W)\n",
        "    print('Please make sure you provide a valid model path and model name before proceeding further.')\n",
        "\n",
        "os.makedirs(Result_folder, exist_ok=True)\n",
        "\n",
        "# Activate the (pre-)trained model\n",
        "if not Use_the_current_trained_model:\n",
        "    model = UNet3D.from_path(Prediction_model_path, Prediction_model_name)\n",
        "    model.to(DEVICE)\n",
        "\n",
        "# Provide the code for performing predictions and saving them\n",
        "# List Tif images in Source_QC_folder\n",
        "Source_QC_folder_tif = Data_folder+\"/*.tif\"\n",
        "Z = sorted(glob.glob(Source_QC_folder_tif))\n",
        "print('Number of streams found in the folder: '+str(len(Z)))\n",
        "for source_file in tqdm(Z, desc=\"Streams\"):\n",
        "\n",
        "    stream_raw = tifffile.imread(source_file)\n",
        "    if stream_raw.ndim != 3:\n",
        "        print(\"[!!!!] File does not appear to be a stream\")\n",
        "        print(f\"[----] {source_file}\")\n",
        "        continue\n",
        "    stream = preprocess_stream(stream_raw)\n",
        "\n",
        "    prediction = model.predict_stream(\n",
        "        {\"input\" : stream, \"shape2crop\" : numpy.array([patch_size * 2, patch_size * 2, patch_size * 2])},\n",
        "        batch_size = batch_size * 2,\n",
        "        step = (patch_size, patch_size, patch_size),\n",
        "        num_workers = 0,\n",
        "        device = DEVICE\n",
        "    )\n",
        "\n",
        "    savename = os.path.splitext(os.path.basename(source_file))[0] + '_prediction.tif'\n",
        "    tifffile.imwrite(\n",
        "        os.path.join(Result_folder, savename), prediction.astype(numpy.float32)\n",
        "    )\n",
        "\n",
        "    del stream_raw, stream, prediction\n",
        "\n",
        "print(\"Images saved into folder:\", Result_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnSk14AJkRtJ"
      },
      "source": [
        "## **6.2. Inspect the predicted output**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hlkZUhj4kQ2Z"
      },
      "outputs": [],
      "source": [
        "# @markdown ##Run this cell to display a randomly chosen input and its corresponding predicted output.\n",
        "\n",
        "# This will display a randomly chosen dataset input and predicted output\n",
        "predicted_files = glob.glob(Result_folder+\"/*.tif\")\n",
        "test_prediction = random.choice(predicted_files)\n",
        "test_source = test_prediction.replace(Result_folder, Data_folder).replace(\"_prediction.tif\", \".tif\")\n",
        "\n",
        "test_source = tifffile.imread(test_source)\n",
        "test_prediction = tifffile.imread(test_prediction)\n",
        "\n",
        "def scroll_in_z(z):\n",
        "\n",
        "    pyplot.figure(figsize=(25,5))\n",
        "    # Source\n",
        "    pyplot.subplot(1,3,1)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(test_source[z-1], cmap='gray')\n",
        "    pyplot.title('Source (z = ' + str(z) + ')', fontsize=15)\n",
        "\n",
        "    # Prediction\n",
        "    pyplot.subplot(1,3,2)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(test_prediction[z-1], cmap='magma', vmin=0, vmax=1)\n",
        "    pyplot.title('Prediction (z = ' + str(z) + ')', fontsize=15)\n",
        "\n",
        "    # Overlay\n",
        "    pyplot.subplot(1,3,3)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(test_source[z-1], cmap='gray')\n",
        "    pyplot.imshow(test_prediction[z-1], alpha=0.5, cmap='magma', vmin=0, vmax=1)\n",
        "    pyplot.title('Overlay (z = ' + str(z) + ')', fontsize=15)\n",
        "    pyplot.show()\n",
        "\n",
        "interact(scroll_in_z, z=widgets.IntSlider(min=1, max=test_source.shape[0], step=1, value=0));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP7WDm6bkYkb"
      },
      "source": [
        "## **6.3. Download your predictions**\n",
        "---\n",
        "\n",
        "<font size = 4>**Store your data** and ALL its results elsewhere by downloading it from Google Drive and after that clean the original folder tree (datasets, results, trained model etc.) if you plan to train or use new networks. Please note that the notebook will otherwise **OVERWRITE** all files which have the same name."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owyvVA3ndrwA"
      },
      "source": [
        "# **7. Version log**\n",
        "---\n",
        "<font size = 4>**v1.0.0**:  \n",
        "\n",
        "\n",
        "*   Initial version of the notebook\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbOn8U-VkerU"
      },
      "source": [
        "\n",
        "# **Thank you for using Calcium U-Net3D**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
